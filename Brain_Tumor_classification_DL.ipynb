{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "vncDsAP0Gaoa",
        "FJNUwmbgGyua",
        "w6K7xa23Elo4",
        "yQaldy8SH6Dl",
        "mDgbUHAGgjLW",
        "O_i_v8NEhb9l",
        "HhfV-JJviCcP",
        "Y3lxredqlCYt",
        "3RnN4peoiCZX",
        "x71ZqKXriCWQ",
        "7hBIi_osiCS2",
        "JlHwYmJAmNHm",
        "35m5QtbWiB9F",
        "PoPl-ycgm1ru",
        "H0kj-8xxnORC",
        "nA9Y7ga8ng1Z",
        "PBTbrJXOngz2",
        "u3PMJOP6ngxN",
        "dauF4eBmngu3",
        "bKJF3rekwFvQ",
        "MSa1f5Uengrz",
        "GF8Ens_Soomf",
        "0wOQAZs5pc--",
        "K5QZ13OEpz2H",
        "lQ7QKXXCp7Bj",
        "448CDAPjqfQr",
        "KSlN3yHqYklG",
        "t6dVpIINYklI",
        "ijmpgYnKYklI",
        "-JiQyfWJYklI",
        "EM7whBJCYoAo",
        "fge-S5ZAYoAp",
        "85gYPyotYoAp",
        "RoGjAbkUYoAp",
        "4Of9eVA-YrdM",
        "iky9q4vBYrdO",
        "F6T5p64dYrdO",
        "y-Ehk30pYrdP",
        "bamQiAODYuh1",
        "QHF8YVU7Yuh3",
        "GwzvFGzlYuh3",
        "qYpmQ266Yuh3",
        "OH-pJp9IphqM",
        "bbFf2-_FphqN",
        "_ouA3fa0phqN",
        "Seke61FWphqN",
        "PIIx-8_IphqN",
        "t27r6nlMphqO",
        "r2jJGEOYphqO",
        "b0JNsNcRphqO",
        "BZR9WyysphqO",
        "jj7wYXLtphqO",
        "eZrbJ2SmphqO",
        "rFu4xreNphqO",
        "YJ55k-q6phqO",
        "gCFgpxoyphqP",
        "OVtJsKN_phqQ",
        "lssrdh5qphqQ",
        "U2RJ9gkRphqQ",
        "1M8mcRywphqQ",
        "tgIPom80phqQ",
        "JMzcOPDDphqR",
        "x-EpHcCOp1ci",
        "X_VqEhTip1ck",
        "8zGJKyg5p1ck",
        "PVzmfK_Ep1ck",
        "n3dbpmDWp1ck",
        "ylSl6qgtp1ck",
        "ZWILFDl5p1ck",
        "M7G43BXep1ck",
        "Ag9LCva-p1cl",
        "E6MkPsBcp1cl",
        "2cELzS2fp1cl",
        "3MPXvC8up1cl",
        "NC_X3p0fY2L0",
        "UV0SzAkaZNRQ",
        "YPEH6qLeZNRQ",
        "q29F0dvdveiT",
        "EXh0U9oCveiU",
        "22aHeOlLveiV",
        "g-ATYxFrGrvw",
        "Yfr_Vlr8HBkt",
        "8yEUt7NnHlrM",
        "tEA2Xm5dHt1r",
        "I79__PHVH19G",
        "Ou-I18pAyIpj",
        "fF3858GYyt-u",
        "4_0_7-oCpUZd",
        "hwyV_J3ipUZe",
        "3yB-zSqbpUZe",
        "dEUvejAfpUZe",
        "Fd15vwWVpUZf",
        "bn_IUdTipZyH",
        "49K5P_iCpZyH",
        "Nff-vKELpZyI",
        "kLW572S8pZyI",
        "dWbDXHzopZyI",
        "yLjJCtPM0KBk",
        "xiyOF9F70UgQ",
        "7wuGOrhz0itI",
        "id1riN9m0vUs",
        "578E2V7j08f6",
        "89xtkJwZ18nB",
        "67NQN5KX2AMe",
        "Iwf50b-R2tYG",
        "GMQiZwjn3iu7",
        "WVIkgGqN3qsr",
        "XkPnILGE3zoT",
        "Hlsf0x5436Go",
        "mT9DMSJo4nBL",
        "c49ITxTc407N",
        "OeJFEK0N496M",
        "9ExmJH0g5HBk",
        "cJNqERVU536h",
        "k5UmGsbsOxih",
        "T0VqWOYE6DLQ",
        "qBMux9mC6MCf",
        "-oLEiFgy-5Pf",
        "C74aWNz2AliB",
        "2DejudWSA-a0",
        "pEMng2IbBLp7",
        "rAdphbQ9Bhjc",
        "TNVZ9zx19K6k",
        "nqoHp30x9hH9",
        "rMDnDkt2B6du",
        "yiiVWRdJDDil",
        "1UUpS68QDMuG",
        "kexQrXU-DjzY",
        "T5CmagL3EC8N",
        "BhH2vgX9EjGr",
        "qjKvONjwE8ra",
        "P1XJ9OREExlT",
        "VFOzZv6IFROw",
        "TIqpNgepFxVj",
        "VfCC591jGiD4",
        "OB4l2ZhMeS1U",
        "ArJBuiUVfxKd",
        "4qY1EAkEfxKe",
        "PiV4Ypx8fxKe",
        "TfvqoZmBfxKf",
        "dJ2tPlVmpsJ0",
        "JWYfwnehpsJ1",
        "-jK_YjpMpsJ2",
        "HAih1iBOpsJ2",
        "zVGeBEFhpsJ2",
        "bmKjuQ-FpsJ3",
        "Fze-IPXLpx6K",
        "7AN1z2sKpx6M",
        "9PIHJqyupx6M",
        "_-qAgymDpx6N",
        "Z-hykwinpx6N",
        "h_CCil-SKHpo",
        "cBFFvTBNJzUa",
        "HvGl1hHyA_VK",
        "EyNgTHvd2WFk",
        "KH5McJBi2d8v",
        "iW_Lq9qf2h6X",
        "-Kee-DAl2viO",
        "gCX9965dhzqZ",
        "gIfDvo9L0UH2"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    - Brain Tumor MRI Image Classification\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Classification\n",
        "##### **Contribution**    - Individual\n",
        "##### **Contributor -** UNNIMAYA K\n"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "## üß† Brain Tumor MRI Image Classification ‚Äì Project Summary\n",
        "\n",
        "### üìå Overview\n",
        "\n",
        "Brain tumors are life-threatening conditions that require early diagnosis for effective treatment. Radiologists typically rely on MRI (Magnetic Resonance Imaging) scans for tumor detection and classification. However, manual analysis can be time-consuming, prone to human error, and resource-intensive‚Äîespecially in remote areas or during a healthcare surge. This project presents a deep learning-based solution to **automate the classification of brain MRI images** into distinct tumor types using both a **custom-built CNN** and **transfer learning** approaches. The final solution is deployed as a **Streamlit web application** to allow seamless real-time predictions for medical professionals and researchers.\n",
        "\n",
        "---\n",
        "\n",
        "### üéØ Objective\n",
        "\n",
        "The primary aim is to build a robust, accurate, and scalable machine learning pipeline that can:\n",
        "\n",
        "* Classify brain MRI images into predefined categories: **Glioma Tumor**, **Meningioma Tumor**, **Pituitary Tumor**, and **No Tumor**.\n",
        "* Leverage both **custom convolutional neural networks** and **pretrained transfer learning models**.\n",
        "* Provide an interactive and accessible **Streamlit interface** for end-users to upload images and receive instant classification results.\n",
        "\n",
        "---\n",
        "\n",
        "### üßæ Dataset Description\n",
        "\n",
        "The dataset used is the **Brain Tumor MRI Multi-Class Dataset**, which contains thousands of labeled MRI images categorized into four classes. The images differ in quality, brightness, orientation, and resolution, which introduces real-world variability. Thus, preprocessing and augmentation become essential steps in the pipeline.\n",
        "\n",
        "---\n",
        "\n",
        "### üõ†Ô∏è Project Workflow\n",
        "\n",
        "#### 1. **Data Understanding**\n",
        "\n",
        "* Explored sample images from each class.\n",
        "* Analyzed the class distribution to detect any **class imbalance**.\n",
        "* Identified inconsistencies in image resolution and structure.\n",
        "\n",
        "#### 2. **Data Preprocessing & Augmentation**\n",
        "\n",
        "* Resized all images to a standard input shape (e.g., **224√ó224 pixels**).\n",
        "* Normalized pixel values to the 0‚Äì1 range.\n",
        "* Applied **data augmentation** (rotation, flip, zoom, brightness shift) to enhance generalization and combat overfitting.\n",
        "\n",
        "#### 3. **Model Building**\n",
        "\n",
        "* **Custom CNN**: Designed from scratch with convolutional layers, pooling, batch normalization, and dropout layers to minimize overfitting.\n",
        "* **Transfer Learning**: Used pretrained models like **ResNet50**, **EfficientNetB0**, and **MobileNetV2**, replacing top layers to adapt them to the four tumor classes.\n",
        "* Used `EarlyStopping`, `ModelCheckpoint`, and validation monitoring during training.\n",
        "\n",
        "#### 4. **Model Evaluation**\n",
        "\n",
        "* Evaluated model performance on unseen test data using:\n",
        "\n",
        "  * **Accuracy**\n",
        "  * **Precision**, **Recall**, and **F1-score**\n",
        "  * **Confusion Matrix**\n",
        "* Visualized training history (accuracy/loss curves) to detect overfitting or underfitting.\n",
        "\n",
        "#### 5. **Model Comparison**\n",
        "\n",
        "* The pretrained models outperformed the custom CNN in terms of accuracy and training speed.\n",
        "* **MobileNetV2** was selected as the final model due to its **high accuracy, smaller size**, and **fast inference speed**.\n",
        "\n",
        "#### 6. **Deployment with Streamlit**\n",
        "\n",
        "* Built a responsive **Streamlit web app** where users can upload MRI images.\n",
        "* Displays the predicted tumor class and model confidence.\n",
        "* The UI is simple, intuitive, and accessible to non-technical users (e.g., medical practitioners).\n",
        "\n",
        "---\n",
        "\n",
        "### üìà Key Insights\n",
        "\n",
        "* Transfer learning significantly boosts model performance, especially with limited labeled data.\n",
        "* Data augmentation is essential for preventing overfitting and ensuring model generalization.\n",
        "* Early detection of brain tumors via automated classification can support radiologists, especially in resource-constrained settings.\n",
        "\n",
        "---\n",
        "\n",
        "### üì¶ Deliverables\n",
        "\n",
        "* `.h5` files for trained models.\n",
        "* Jupyter notebooks/scripts for preprocessing, training, evaluation, and deployment.\n",
        "* Streamlit app script (`app.py`) for web-based prediction.\n",
        "* Documentation including README, performance plots, and insights.\n",
        "\n",
        "---\n",
        "\n",
        "### üí° Business Impact\n",
        "\n",
        "* **Faster diagnosis**: Reduces diagnostic delay and speeds up clinical decisions.\n",
        "* **Scalability**: Easily deployable in hospitals, research labs, and rural health centers.\n",
        "* **Cost-effective**: Automates repetitive image classification tasks to reduce labor costs.\n",
        "* **Supports telemedicine**: Helps doctors in remote areas or with limited access to radiologists.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Provide your GitHub Link here."
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Brain tumors are among the most life-threatening and challenging medical conditions to diagnose and treat. The traditional diagnosis process, which relies heavily on radiologists interpreting MRI (Magnetic Resonance Imaging) scans, can be time-consuming, prone to human error, and limited by the availability of medical experts‚Äîespecially in under-resourced regions. Early and accurate classification of brain tumors is critical for timely intervention and improved patient outcomes.\n",
        "\n",
        "This project aims to develop an AI-powered image classification system using deep learning techniques to automatically classify brain MRI scans into multiple tumor types, including Glioma, Meningioma, Pituitary Tumor, and No Tumor. The solution should be accurate, scalable, and user-friendly, supporting radiologists and medical professionals in making faster and more informed decisions.\n",
        "\n",
        "The system will be trained using both custom CNN architectures and pretrained transfer learning models, evaluated for performance, and finally deployed using Streamlit as an interactive web application. By automating the tumor classification process, this project seeks to reduce the diagnostic burden on medical professionals and enhance healthcare delivery‚Äîespecially in regions lacking specialized medical expertise.."
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Import Libraries**"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "# üì¶ Basic Libraries\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# üñº Image Handling\n",
        "import cv2\n",
        "from PIL import Image\n",
        "\n",
        "# üîÅ Data Preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# üß† Deep Learning Libraries (TensorFlow & Keras)\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.applications import ResNet50, MobileNetV2, InceptionV3, EfficientNetB0\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input as resnet_preprocess\n",
        "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input as mobilenet_preprocess\n",
        "from tensorflow.keras.applications.inception_v3 import preprocess_input as inception_preprocess\n",
        "from tensorflow.keras.applications.efficientnet import preprocess_input as efficientnet_preprocess\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Input\n",
        "\n",
        "# ‚úÖ Suppress warnings and logs for clean output\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "tf.get_logger().setLevel('ERROR')\n"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Dataset Loading**"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####***1. Mount Google Drive***"
      ],
      "metadata": {
        "id": "0vLzhw7ZHtEv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive to access dataset and save models\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üìò This allows you to load datasets and save outputs directly to your Google Drive.\n",
        "\n"
      ],
      "metadata": {
        "id": "cUNIYLl3HvI-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####***üìå 2. Set Dataset Path and Show Classes***\n"
      ],
      "metadata": {
        "id": "zxfVUQZ-H92g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "dataset_path = \"/content/drive/MyDrive/Tumour\"\n",
        "print(\"Classes:\", os.listdir(dataset_path))\n"
      ],
      "metadata": {
        "id": "t-wXyb9tdSpl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üìò Confirms that the dataset is structured into 'train', 'valid', and 'test' folders.\n"
      ],
      "metadata": {
        "id": "4n_bghzlIEeL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####***üìå 3. Image Augmentation and Data Generator Setup***\n"
      ],
      "metadata": {
        "id": "8nVVv_N5IWZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "valid_datagen = ImageDataGenerator(rescale=1./255)"
      ],
      "metadata": {
        "id": "JfEF5osnoeFt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üìò Improves model generalization and helps fight overfitting."
      ],
      "metadata": {
        "id": "kbukLkX0IbC7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####***üìå 4. Load Training and Validation Data***\n"
      ],
      "metadata": {
        "id": "xZf_xobZIela"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define paths\n",
        "base_path = \"/content/drive/MyDrive/Tumour\"\n",
        "train_path = os.path.join(base_path, \"train\")\n",
        "valid_path = os.path.join(base_path, \"valid\")\n",
        "test_path = os.path.join(base_path, \"test\")\n"
      ],
      "metadata": {
        "id": "tIVCZfojLbsn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_set = train_datagen.flow_from_directory(\n",
        "    train_path,\n",
        "    target_size=(128, 128),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")"
      ],
      "metadata": {
        "id": "Dqd-o9lRouIX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "validation_set = valid_datagen.flow_from_directory(\n",
        "    valid_path,\n",
        "    target_size=(128, 128),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n"
      ],
      "metadata": {
        "id": "j5MlBZeuo-WV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = training_set.num_classes  # Dynamically get number of classes"
      ],
      "metadata": {
        "id": "VyHSCX4GpHdh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üìò Automatically loads image data and creates batches ready for training."
      ],
      "metadata": {
        "id": "XU_yMGLvIwsz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####***üìå 5. Build Model using Transfer Learning***\n"
      ],
      "metadata": {
        "id": "JNCHk-6hI7UF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dropout, Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n"
      ],
      "metadata": {
        "id": "R78wGHXPp2TQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**Training using Custom CNN**"
      ],
      "metadata": {
        "id": "IBCAEYzS7UNF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "\n",
        "# üî∏ Stop training if no improvement in val_loss for 3 epochs\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=3,\n",
        "    restore_best_weights=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# üî∏ Reduce learning rate if val_loss plateaus\n",
        "lr_scheduler = ReduceLROnPlateau(\n",
        "    monitor='val_loss',\n",
        "    factor=0.1,\n",
        "    patience=2,\n",
        "    min_lr=1e-6,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Optional: save best model during training (file name/path can be changed)\n",
        "checkpoint = ModelCheckpoint(\n",
        "    'best_model_custom_cnn.h5',\n",
        "    monitor='val_loss',\n",
        "    save_best_only=True,\n",
        "    verbose=1\n",
        ")\n"
      ],
      "metadata": {
        "id": "F7G8HmQS9EE6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# üîπ Define Custom CNN Model\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
        "\n",
        "num_classes = training_set.num_classes  # auto-detect from your dataset\n",
        "\n",
        "custom_cnn = Sequential([\n",
        "    Conv2D(32, (3,3), activation='relu', input_shape=(128,128,3)),\n",
        "    MaxPooling2D(2,2),\n",
        "    BatchNormalization(),\n",
        "\n",
        "    Conv2D(64, (3,3), activation='relu'),\n",
        "    MaxPooling2D(2,2),\n",
        "    BatchNormalization(),\n",
        "\n",
        "    Conv2D(128, (3,3), activation='relu'),\n",
        "    MaxPooling2D(2,2),\n",
        "    Flatten(),\n",
        "\n",
        "    Dense(256, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "custom_cnn.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "history_custom = custom_cnn.fit(\n",
        "    training_set,\n",
        "    validation_data=validation_set,\n",
        "    epochs=10,\n",
        "    callbacks=[early_stopping, lr_scheduler, checkpoint]\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "sX3CsPuD7SFz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**Training using MobileNetV2**"
      ],
      "metadata": {
        "id": "elX7bPvB7ZFi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Pretrained Model\n",
        "base_model = MobileNetV2(input_shape=(128, 128, 3), include_top=False, weights='imagenet')\n",
        "base_model.trainable = False  # Freeze initial layers"
      ],
      "metadata": {
        "id": "2zYd5RWwp45m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define the Model\n",
        "cnn = Sequential([\n",
        "    base_model,\n",
        "    GlobalAveragePooling2D(),\n",
        "    Dense(512, activation='relu'),\n",
        "    Dropout(0.4),\n",
        "    Dense(num_classes, activation='softmax')\n",
        "])"
      ],
      "metadata": {
        "id": "pu_5rxs3p8Qe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üìò Uses a pre-trained MobileNetV2 and custom classification head for our dataset."
      ],
      "metadata": {
        "id": "ZHpjUxJfJAmF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####***üìå 6. Compile and Train the Model***"
      ],
      "metadata": {
        "id": "h49aiOqRJFq5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Compile the Model\n",
        "cnn.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
        "            loss='categorical_crossentropy',\n",
        "            metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "mOYNT6QXqATA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Callbacks\n",
        "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)"
      ],
      "metadata": {
        "id": "NsDp9HPkqBO9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "training_history = cnn.fit(training_set,\n",
        "                           validation_data=validation_set,\n",
        "                           epochs=20,\n",
        "                           callbacks=[early_stopping, lr_scheduler])"
      ],
      "metadata": {
        "id": "ienMPle2qEn0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üìò Uses early stopping and learning rate scheduler to optimize training."
      ],
      "metadata": {
        "id": "io2nUw9vJJum"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####***üìå 7. Evaluate Model Performance***"
      ],
      "metadata": {
        "id": "k501MDgkJPzs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Evaluate Model Performance\n",
        "train_loss, train_acc = cnn.evaluate(training_set)\n",
        "val_loss, val_acc = cnn.evaluate(validation_set)\n",
        "print(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc:.4f}\")\n",
        "print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_acc:.4f}\")"
      ],
      "metadata": {
        "id": "H8y38ubaqKuN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Confusion Matrix of Custom CNN"
      ],
      "metadata": {
        "id": "YDj0Qa4Z72ZC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Get a batch from validation set\n",
        "val_images, val_labels = next(iter(validation_set))\n",
        "\n",
        "# Predict using custom CNN\n",
        "y_pred_custom = np.argmax(custom_cnn.predict(val_images), axis=1)\n",
        "y_true_custom = np.argmax(val_labels, axis=1)\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_true_custom, y_pred_custom)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=list(training_set.class_indices.keys()))\n",
        "disp.plot(cmap='Blues')\n",
        "plt.title(\"Confusion Matrix - Custom CNN\")\n",
        "plt.show()\n",
        "\n",
        "# Classification Report\n",
        "print(\"üîπ Classification Report (Custom CNN):\\n\")\n",
        "print(classification_report(y_true_custom, y_pred_custom, target_names=list(training_set.class_indices.keys())))\n"
      ],
      "metadata": {
        "id": "Yk7olcTS7yEn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Classification Report for MobileNetV2"
      ],
      "metadata": {
        "id": "a6diuWJnJAN4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_mob = np.argmax(cnn.predict(val_images), axis=1)\n",
        "print(classification_report(y_true_custom, y_pred_mob, target_names=list(training_set.class_indices.keys())))\n"
      ],
      "metadata": {
        "id": "V8Y-DRXk8D42"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####***8 .Comaprison of the Models***\n",
        "\n",
        "##üîç Classification Report Analysis\n",
        "\n",
        "##üìå Custom CNN Results\n",
        "\n",
        "| Metric       | Value                                                                                                                                      |\n",
        "| ------------ | ------------------------------------------------------------------------------------------------------------------------------------------ |\n",
        "| **Accuracy** | 19%                                                                                                                                        |\n",
        "| **F1-Score** | 0.08 (macro)                                                                                                                               |\n",
        "| **Insight**  | Only the *Pituitary* class is being predicted (and not very well). The rest have **0 recall**, meaning they're never correctly identified. |\n",
        "| ‚ö†Ô∏è **Issue** | Model is severely underfitting or unable to learn class distinctions.                                                                      |\n",
        "\n",
        "##‚úÖ MobileNetV2 Results\n",
        "\n",
        "\n",
        "| Metric             | Value                                                                                     |\n",
        "| ------------------ | ----------------------------------------------------------------------------------------- |\n",
        "| **Accuracy**       | 91%                                                                                       |\n",
        "| **F1-Score**       | 0.81 (macro)                                                                              |\n",
        "| **Insight**        | Excellent performance. All classes except *Meningioma* are predicted well.                |\n",
        "| ‚ö†Ô∏è **Minor Issue** | *Meningioma* has low recall (0.25). Possibly due to class imbalance or insufficient data. |\n"
      ],
      "metadata": {
        "id": "fealr-hZGIUF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####***üìå 9. Save Model and Training History***"
      ],
      "metadata": {
        "id": "iqEkS8KEJWNK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Save Training History\n",
        "import json\n",
        "with open('/content/drive/MyDrive/tumor_training_hist.json', 'w') as f:\n",
        "    json.dump(training_history.history, f)\n",
        "\n",
        "print(training_history.history.keys())"
      ],
      "metadata": {
        "id": "AZmhjaV9qNf9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Save the Model\n",
        "cnn.save('/content/drive/MyDrive/trained_braintumor_disease_model.keras')"
      ],
      "metadata": {
        "id": "Xu036RSAqRCd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn.save(\"/content/drive/MyDrive/trained_braintumor_disease_model.h5\")"
      ],
      "metadata": {
        "id": "uLL5IW1oqSAt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####***üìå 10. Visualize Training Curves***\n"
      ],
      "metadata": {
        "id": "xUyEWUTnJfrQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Plot Accuracy Curves\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Extract number of epochs from history\n",
        "num_epochs = len(training_history.history['accuracy'])\n",
        "epochs = list(range(1, num_epochs + 1))\n",
        "\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(epochs, training_history.history['accuracy'], color='red', label='Training Accuracy')\n",
        "plt.plot(epochs, training_history.history['val_accuracy'], color='blue', label='Validation Accuracy')\n",
        "plt.xlabel('No. of Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Accuracy Visualization')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "0IIojNaoqXPM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "history = training_history.history\n",
        "epochs = list(range(1, len(history['accuracy']) + 1))\n",
        "\n",
        "# 1. Accuracy Plot\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(epochs, history['accuracy'], 'r-', label='Training Accuracy')\n",
        "plt.plot(epochs, history['val_accuracy'], 'b-', label='Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('1Ô∏è‚É£ Training vs Validation Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# 2. Loss Plot\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(epochs, history['loss'], 'orange', label='Training Loss')\n",
        "plt.plot(epochs, history['val_loss'], 'green', label='Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('2Ô∏è‚É£ Training vs Validation Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# 3. Accuracy & Loss Together\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.plot(epochs, history['accuracy'], 'r--', label='Train Accuracy')\n",
        "plt.plot(epochs, history['val_accuracy'], 'b--', label='Val Accuracy')\n",
        "plt.plot(epochs, history['loss'], 'r-', label='Train Loss')\n",
        "plt.plot(epochs, history['val_loss'], 'b-', label='Val Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.title('3Ô∏è‚É£ Accuracy & Loss Together')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# 4. Zoomed-in Validation Accuracy\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(epochs, history['val_accuracy'], color='purple', marker='o')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Validation Accuracy')\n",
        "plt.title('4Ô∏è‚É£ Zoomed-in Validation Accuracy')\n",
        "plt.ylim(min(history['val_accuracy']) - 0.05, max(history['val_accuracy']) + 0.05)\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# 5. Accuracy Gain Between Epochs\n",
        "accuracy_gain = [history['accuracy'][i] - history['accuracy'][i-1] for i in range(1, len(epochs))]\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.bar(range(2, len(epochs)+1), accuracy_gain, color='teal')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy Gain')\n",
        "plt.title('5Ô∏è‚É£ Accuracy Gain Per Epoch (Train Set)')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "klgNgLga_01r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**To Compare the validation Accuracy of the two models**"
      ],
      "metadata": {
        "id": "-lVwp5h08PHu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# üî∏ Compare Validation Accuracy\n",
        "val_acc_custom = history_custom.history['val_accuracy'][-1]\n",
        "val_acc_transfer = training_history.history['val_accuracy'][-1]\n",
        "\n",
        "print(f\"‚úÖ Validation Accuracy - Custom CNN: {val_acc_custom:.4f}\")\n",
        "print(f\"‚úÖ Validation Accuracy - MobileNetV2: {val_acc_transfer:.4f}\")\n",
        "\n",
        "# üî∏ Accuracy Visualization\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(history_custom.history['val_accuracy'], label='Custom CNN', color='green')\n",
        "plt.plot(training_history.history['val_accuracy'], label='MobileNetV2', color='orange')\n",
        "plt.title(\"Model Comparison - Validation Accuracy\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Validation Accuracy\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "fHy2mhaW8OJa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####***üìå 11. Load Model and Test on New Image***"
      ],
      "metadata": {
        "id": "j8iMuu6kJn4t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Model for Testing\n",
        "cnn = tf.keras.models.load_model('/content/drive/MyDrive/trained_braintumor_disease_model.keras')\n",
        "cnn.summary()"
      ],
      "metadata": {
        "id": "PO0CMoGiqc5B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Loading Model from the drive\n",
        "import tensorflow as tf\n",
        "\n",
        "try:\n",
        "    model = tf.keras.models.load_model(\"/content/drive/MyDrive/trained_braintumor_disease_model.keras\")\n",
        "    print(\"‚úÖ Model loaded successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"üö® Error loading model: {e}\")"
      ],
      "metadata": {
        "id": "Xazrb8lAqnIa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Install OpenCV (Only if needed)\n",
        "try:\n",
        "    import cv2\n",
        "except ImportError:\n",
        "    %pip install opencv-python\n",
        "    import cv2"
      ],
      "metadata": {
        "id": "DiinpLDLqoEC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test Image Path\n",
        "image_path = '/content/drive/MyDrive/Tumour/test/pituitary/Tr-pi_0090_jpg.rf.c08e55649aa763919fefc64964f2e6b4.jpg'"
      ],
      "metadata": {
        "id": "nU32vwBeqq0S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import matplotlib.pyplot as plt  # Correct import\n",
        "  # Update with actual image path\n",
        "\n",
        "# Check if the file exists before loading\n",
        "import os\n",
        "if not os.path.exists(image_path):\n",
        "    print(\"Error: Image file not found!\")\n",
        "else:\n",
        "    img = cv2.imread(image_path)\n",
        "\n",
        "    if img is None:\n",
        "        print(\"Error: OpenCV couldn't read the image!\")\n",
        "    else:\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB\n",
        "        plt.imshow(img)  # Show image\n",
        "        plt.title('Test Image')\n",
        "        plt.xticks([])\n",
        "        plt.yticks([])\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "XeS6SL1PqtR3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load and Preprocess Test Image\n",
        "import numpy as np\n",
        "image = tf.keras.preprocessing.image.load_img(image_path, target_size=(128,128))\n",
        "input_arr = tf.keras.preprocessing.image.img_to_array(image)\n",
        "input_arr = np.expand_dims(input_arr, axis=0) / 255.0  # Normalize"
      ],
      "metadata": {
        "id": "OaBwujuLqyhu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Softmax with Temperature Scaling\n",
        "def softmax_with_temperature(logits, temperature=2.0):\n",
        "    exp_logits = np.exp(logits / temperature)\n",
        "    return exp_logits / np.sum(exp_logits, axis=1, keepdims=True)"
      ],
      "metadata": {
        "id": "xijt-pQuqze0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make Predictions\n",
        "predictions = cnn.predict(input_arr)  # Get raw logits\n",
        "scaled_predictions = softmax_with_temperature(predictions, temperature=2.0)  # Apply Temperature Scaling\n"
      ],
      "metadata": {
        "id": "rM5TeuGjq2KX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get Class Labels\n",
        "class_labels = list(training_set.class_indices.keys())  # Ensure class names are retrieved properly"
      ],
      "metadata": {
        "id": "zSaJy_Gzq4bP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display Prediction\n",
        "result_index = np.argmax(scaled_predictions)\n",
        "print(\"Predicted Class:\", class_labels[result_index])"
      ],
      "metadata": {
        "id": "nVqxoFMgq7sT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Displaying the disease prediction\n",
        "model_prediction = class_labels[result_index]\n",
        "plt.imshow(img)\n",
        "plt.title(f\"Disease Name: {model_prediction}\")\n",
        "plt.xticks([])\n",
        "plt.yticks([])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "X4fUqxHprAEe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####***12. Streamlit Implementation***"
      ],
      "metadata": {
        "id": "OcTMoT4ZJ7X-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install streamlit tensorflow"
      ],
      "metadata": {
        "id": "O9W140fnrCtK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install pyngrok"
      ],
      "metadata": {
        "id": "Wgkp5MTsrDi0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####App code"
      ],
      "metadata": {
        "id": "qXIBLhnHLeCU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "# Load model\n",
        "@st.cache_resource\n",
        "def load_model():\n",
        "    model = tf.keras.models.load_model(\"/content/drive/MyDrive/trained_braintumor_disease_model.h5\")\n",
        "    return model\n",
        "\n",
        "model = load_model()\n",
        "\n",
        "# Class names (update if different)\n",
        "class_names = ['Glioma', 'Meningioma', 'No Tumor', 'Pituitary']\n",
        "\n",
        "st.set_page_config(page_title=\"Brain Tumor Classifier\", layout=\"centered\")\n",
        "st.title(\"üß† Brain Tumor MRI Image Classifier\")\n",
        "st.markdown(\"Upload a brain MRI image to predict tumor type.\")\n",
        "\n",
        "uploaded_file = st.file_uploader(\"Upload MRI Image\", type=[\"jpg\", \"png\", \"jpeg\"])\n",
        "\n",
        "if uploaded_file is not None:\n",
        "    image = Image.open(uploaded_file).convert('RGB')\n",
        "    st.image(image, caption='Uploaded MRI Image', use_column_width=True)\n",
        "\n",
        "    # ‚úÖ Resize to match model input shape (128x128)\n",
        "    img = image.resize((128, 128))\n",
        "    img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
        "    img_array = np.expand_dims(img_array, axis=0)\n",
        "    img_array = img_array / 255.0  # normalize\n",
        "\n",
        "    # Predict\n",
        "    predictions = model.predict(img_array)\n",
        "    predicted_class = class_names[np.argmax(predictions)]\n",
        "    confidence = round(100 * np.max(predictions), 2)\n",
        "\n",
        "    st.success(f\"### üß™ Prediction: **{predicted_class}** ({confidence}% confidence)\")\n"
      ],
      "metadata": {
        "id": "k3i8WdtXrGGW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#To get the IP Address\n",
        "!curl ipv4.icanhazip.com"
      ],
      "metadata": {
        "id": "1zxDXLbLrIc9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Paste the output of the above code as the Tunnel Password in the webpage displayed while running the below code to connect through Local Tunnel"
      ],
      "metadata": {
        "id": "mRvVmL4aL5qw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run  app.py & npx localtunnel --port 8501"
      ],
      "metadata": {
        "id": "-70I80XnrKlb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this project, we successfully developed a deep learning solution for brain tumor classification using MRI images. We explored both a Custom Convolutional Neural Network (CNN) and a Transfer Learning approach using MobileNetV2 pretrained on ImageNet.\n",
        "\n",
        "üî¨ Key Findings:\n",
        "The Custom CNN, although designed and trained from scratch, failed to generalize well and achieved a low validation accuracy of 19%, with poor precision and recall for most tumor classes.\n",
        "\n",
        "In contrast, MobileNetV2 delivered excellent performance with 91% validation accuracy and strong classification metrics across most tumor categories, including perfect scores for \"No Tumor\" and high precision for \"Glioma\" and \"Pituitary.\"\n",
        "\n",
        "The only challenge observed was the relatively lower recall for the \"Meningioma\" class, likely due to class imbalance in the dataset.\n",
        "\n",
        "üìà Performance Comparison:\n",
        "Model\tAccuracy\tMacro F1-Score\tRemarks\n",
        "Custom CNN\t19%\t0.08\tUnderfitting, needs improvement\n",
        "MobileNetV2\t91%\t0.81\tBest performing model\n",
        "\n",
        "üõ†Ô∏è Final Implementation:\n",
        "Model Optimization: MobileNetV2 was fine-tuned with additional dense layers and regularization techniques like dropout and batch normalization.\n",
        "\n",
        "Evaluation Metrics: Accuracy, precision, recall, F1-score, and confusion matrix were used to assess performance.\n",
        "\n",
        "Visualization: Multiple plots (accuracy/loss curves, confusion matrix) helped in monitoring model training and performance.\n",
        "\n",
        "Deployment: The best model was integrated into a Streamlit web application, allowing users to upload brain MRI scans and receive tumor predictions with confidence scores.\n",
        "\n",
        "üöÄ Future Scope:\n",
        "Improve meningioma classification by handling class imbalance through techniques like SMOTE or class weighting.\n",
        "\n",
        "Extend the dataset to include more diverse samples and rare tumor subtypes.\n",
        "\n",
        "Incorporate Grad-CAM for visual explainability of model predictions.\n",
        "\n",
        "Deploy the model on cloud platforms (e.g., Heroku, AWS, GCP) for broader accessibility."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}