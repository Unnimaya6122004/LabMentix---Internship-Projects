{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "vncDsAP0Gaoa",
        "FJNUwmbgGyua",
        "w6K7xa23Elo4",
        "yQaldy8SH6Dl",
        "mDgbUHAGgjLW",
        "HhfV-JJviCcP",
        "Y3lxredqlCYt",
        "3RnN4peoiCZX",
        "x71ZqKXriCWQ",
        "7hBIi_osiCS2",
        "JlHwYmJAmNHm",
        "35m5QtbWiB9F",
        "PoPl-ycgm1ru",
        "H0kj-8xxnORC",
        "nA9Y7ga8ng1Z",
        "u3PMJOP6ngxN",
        "dauF4eBmngu3",
        "bKJF3rekwFvQ",
        "GF8Ens_Soomf",
        "0wOQAZs5pc--",
        "K5QZ13OEpz2H",
        "lQ7QKXXCp7Bj",
        "448CDAPjqfQr",
        "KSlN3yHqYklG",
        "t6dVpIINYklI",
        "ijmpgYnKYklI",
        "-JiQyfWJYklI",
        "EM7whBJCYoAo",
        "fge-S5ZAYoAp",
        "85gYPyotYoAp",
        "RoGjAbkUYoAp",
        "4Of9eVA-YrdM",
        "iky9q4vBYrdO",
        "F6T5p64dYrdO",
        "y-Ehk30pYrdP",
        "bamQiAODYuh1",
        "QHF8YVU7Yuh3",
        "GwzvFGzlYuh3",
        "qYpmQ266Yuh3",
        "bbFf2-_FphqN",
        "_ouA3fa0phqN",
        "Seke61FWphqN",
        "PIIx-8_IphqN",
        "t27r6nlMphqO",
        "r2jJGEOYphqO",
        "b0JNsNcRphqO",
        "BZR9WyysphqO",
        "jj7wYXLtphqO",
        "eZrbJ2SmphqO",
        "rFu4xreNphqO",
        "YJ55k-q6phqO",
        "gCFgpxoyphqP",
        "OVtJsKN_phqQ",
        "lssrdh5qphqQ",
        "U2RJ9gkRphqQ",
        "1M8mcRywphqQ",
        "tgIPom80phqQ",
        "JMzcOPDDphqR",
        "x-EpHcCOp1ci",
        "X_VqEhTip1ck",
        "8zGJKyg5p1ck",
        "PVzmfK_Ep1ck",
        "n3dbpmDWp1ck",
        "ylSl6qgtp1ck",
        "ZWILFDl5p1ck",
        "M7G43BXep1ck",
        "Ag9LCva-p1cl",
        "E6MkPsBcp1cl",
        "2cELzS2fp1cl",
        "3MPXvC8up1cl",
        "NC_X3p0fY2L0",
        "UV0SzAkaZNRQ",
        "YPEH6qLeZNRQ",
        "q29F0dvdveiT",
        "EXh0U9oCveiU",
        "22aHeOlLveiV",
        "g-ATYxFrGrvw",
        "Yfr_Vlr8HBkt",
        "8yEUt7NnHlrM",
        "tEA2Xm5dHt1r",
        "I79__PHVH19G",
        "Ou-I18pAyIpj",
        "fF3858GYyt-u",
        "4_0_7-oCpUZd",
        "3yB-zSqbpUZe",
        "bn_IUdTipZyH",
        "Nff-vKELpZyI",
        "yLjJCtPM0KBk",
        "xiyOF9F70UgQ",
        "7wuGOrhz0itI",
        "id1riN9m0vUs",
        "578E2V7j08f6",
        "89xtkJwZ18nB",
        "67NQN5KX2AMe",
        "Iwf50b-R2tYG",
        "GMQiZwjn3iu7",
        "WVIkgGqN3qsr",
        "XkPnILGE3zoT",
        "Hlsf0x5436Go",
        "mT9DMSJo4nBL",
        "c49ITxTc407N",
        "OeJFEK0N496M",
        "9ExmJH0g5HBk",
        "cJNqERVU536h",
        "k5UmGsbsOxih",
        "T0VqWOYE6DLQ",
        "qBMux9mC6MCf",
        "-oLEiFgy-5Pf",
        "C74aWNz2AliB",
        "2DejudWSA-a0",
        "pEMng2IbBLp7",
        "rAdphbQ9Bhjc",
        "TNVZ9zx19K6k",
        "nqoHp30x9hH9",
        "rMDnDkt2B6du",
        "yiiVWRdJDDil",
        "1UUpS68QDMuG",
        "kexQrXU-DjzY",
        "T5CmagL3EC8N",
        "BhH2vgX9EjGr",
        "qjKvONjwE8ra",
        "P1XJ9OREExlT",
        "VFOzZv6IFROw",
        "TIqpNgepFxVj",
        "VfCC591jGiD4",
        "OB4l2ZhMeS1U",
        "ArJBuiUVfxKd",
        "4qY1EAkEfxKe",
        "PiV4Ypx8fxKe",
        "TfvqoZmBfxKf",
        "dJ2tPlVmpsJ0",
        "JWYfwnehpsJ1",
        "bmKjuQ-FpsJ3",
        "h_CCil-SKHpo",
        "cBFFvTBNJzUa",
        "HvGl1hHyA_VK",
        "KH5McJBi2d8v",
        "iW_Lq9qf2h6X",
        "-Kee-DAl2viO",
        "gCX9965dhzqZ",
        "gIfDvo9L0UH2"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    -  **Shopper Spectrum: Customer Segmentation and Product Recommendations in E-Commerce**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### **Project Type**    - EDA + Unsupervised Learning + Recommender System\n",
        "##### **Contribution**    - Individual\n",
        "##### **Team Member  - UNNIMAYA K**\n"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### üõí Shopper Spectrum: Customer Segmentation and Product Recommendations in E-Commerce\n",
        "\n",
        "In the dynamic landscape of e-commerce, understanding customer behavior is crucial for personalizing the shopping experience, improving customer retention, and driving business growth. The \"Shopper Spectrum\" project is designed to harness the power of data science to analyze customer purchase patterns, segment customers based on purchasing behavior, and build a product recommendation engine ‚Äî ultimately enhancing user engagement and sales strategies in an online retail setting.\n",
        "\n",
        "---\n",
        "\n",
        "### üìå Objective\n",
        "\n",
        "The primary objective of this project is twofold:\n",
        "\n",
        "1. **Customer Segmentation**: Identify distinct customer groups using RFM (Recency, Frequency, Monetary) analysis and clustering techniques.\n",
        "2. **Product Recommendation**: Suggest similar products to users based on their interests using collaborative filtering.\n",
        "\n",
        "Together, these solutions aim to support more targeted marketing, improve inventory management, and deliver personalized shopping experiences.\n",
        "\n",
        "---\n",
        "\n",
        "### üìä Dataset Overview\n",
        "\n",
        "The dataset comprises online retail transaction records from 2022 to 2023, including features such as:\n",
        "\n",
        "* **InvoiceNo** (transaction ID)\n",
        "* **StockCode** (product code)\n",
        "* **Description** (product name)\n",
        "* **Quantity**, **UnitPrice**\n",
        "* **InvoiceDate**\n",
        "* **CustomerID**\n",
        "* **Country**\n",
        "\n",
        "The dataset is explored for missing values, duplicates, and incorrect entries. Key preprocessing steps involve filtering out cancelled invoices (those starting with 'C'), and removing rows with zero or negative quantities or prices.\n",
        "\n",
        "---\n",
        "\n",
        "### üìà Exploratory Data Analysis (EDA)\n",
        "\n",
        "EDA reveals insights such as:\n",
        "\n",
        "* Top-selling products\n",
        "* Sales distribution across countries\n",
        "* Monthly purchase trends\n",
        "* Distribution of total spending per customer\n",
        "\n",
        "These insights help identify high-value products and regions, enabling better business decision-making.\n",
        "\n",
        "---\n",
        "\n",
        "### üßÆ Customer Segmentation Using RFM and Clustering\n",
        "\n",
        "RFM analysis provides a powerful way to quantify customer behavior:\n",
        "\n",
        "* **Recency**: Days since the last purchase\n",
        "* **Frequency**: Number of transactions\n",
        "* **Monetary**: Total amount spent\n",
        "\n",
        "The RFM values are scaled using **StandardScaler**. **KMeans clustering** is then applied to group customers into segments based on these metrics. The **Elbow Method** and **Silhouette Score** guide the optimal number of clusters. Each cluster is then labeled with business-friendly tags like:\n",
        "\n",
        "* **High-Value**: Recent, frequent, high-spending customers\n",
        "* **Regular**: Moderate but steady spenders\n",
        "* **Occasional**: Infrequent, low spenders\n",
        "* **At-Risk**: Customers who haven‚Äôt purchased recently\n",
        "\n",
        "This segmentation allows businesses to tailor marketing and retention strategies effectively.\n",
        "\n",
        "---\n",
        "\n",
        "### ü§ù Product Recommendation System\n",
        "\n",
        "An **Item-Based Collaborative Filtering** technique is used for building the recommendation engine:\n",
        "\n",
        "* A Customer‚ÄìProduct matrix is created.\n",
        "* **Cosine similarity** is computed between products based on co-purchases.\n",
        "* When a user enters a product name, the system returns the top 5 most similar items.\n",
        "\n",
        "This enables cross-selling by suggesting items frequently bought together or similar in nature.\n",
        "\n",
        "---\n",
        "\n",
        "### üåê Streamlit Application\n",
        "\n",
        "A user-friendly web interface is built using **Streamlit** with two key modules:\n",
        "\n",
        "1. **Product Recommendation**: Enter a product name and get 5 similar suggestions.\n",
        "2. **Customer Segmentation**: Input Recency, Frequency, and Monetary values to identify the customer cluster.\n",
        "\n",
        "The interface is clean, responsive, and provides real-time output, enhancing user experience.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ Conclusion\n",
        "\n",
        "The \"Shopper Spectrum\" project successfully bridges data science and retail business needs by offering a dual solution of segmentation and recommendation. With comprehensive EDA, insightful clustering, and an intuitive recommendation system, this project empowers e-commerce businesses to make data-driven decisions, boost personalization, and enhance customer satisfaction. It also highlights the value of machine learning and unsupervised techniques in transforming raw transaction data into actionable business intelligence.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Provide your GitHub Link here."
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The e-commerce industry generates massive volumes of customer transaction data daily. However, most businesses struggle to harness this data effectively to understand customer behavior, identify target segments, and offer personalized experiences. Without proper segmentation and recommendation mechanisms, companies risk delivering generic marketing, losing customer engagement, and mismanaging inventory.\n",
        "\n",
        "This project aims to analyze historical transaction data from an online retail store to uncover patterns in customer purchasing behavior. It focuses on two core objectives:\n",
        "\n",
        "1. **Customer Segmentation** ‚Äì Using Recency, Frequency, and Monetary (RFM) analysis combined with clustering algorithms to group customers into meaningful behavioral segments.\n",
        "2. **Product Recommendation** ‚Äì Implementing an item-based collaborative filtering system to suggest similar products based on purchase history.\n",
        "\n",
        "By addressing these areas, the project provides e-commerce businesses with actionable insights for targeted marketing, product promotion, customer retention strategies, and personalized shopping experiences ‚Äî ultimately enhancing both customer satisfaction and business performance.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "# üì¶ Data Manipulation & Handling\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# üìà Data Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "# üìä Date and Time Handling\n",
        "from datetime import datetime\n",
        "\n",
        "# üßπ Data Preprocessing & Scaling\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# üìâ Clustering Algorithms & Evaluation\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# üß† Recommendation System\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# üõ†Ô∏è Utility\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# üìÅ If using Google Colab, run this to upload files\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n"
      ],
      "metadata": {
        "id": "CLlJ5vdSU0JH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "ec5f7d64-968a-4c96-e972-a46696205e95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-7c4c631e-e0c4-41e7-a1ed-7dbccd78297b\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-7c4c631e-e0c4-41e7-a1ed-7dbccd78297b\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving online_retail.csv to online_retail.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# üîΩ Load Dataset\n",
        "file_path = 'online_retail.csv'  # Change the filename if needed\n",
        "\n",
        "# Load the dataset into a DataFrame\n",
        "df = pd.read_csv(file_path, encoding='ISO-8859-1')  # Use correct encoding to avoid issues\n",
        "\n",
        "# Display basic information\n",
        "print(\"Shape of dataset:\", df.shape)\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "# üîç Dataset First Look\n",
        "\n",
        "# Check the first few rows\n",
        "df.head()\n",
        "\n",
        "# Check the column names and datatypes\n",
        "print(\"\\nüìå Dataset Info:\")\n",
        "df.info()\n",
        "\n",
        "# Check for missing values\n",
        "print(\"\\n‚ùì Missing Values:\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# Check for duplicates\n",
        "print(\"\\nüìå Duplicate Rows:\", df.duplicated().sum())\n",
        "\n",
        "# Check basic statistics\n",
        "print(\"\\nüìä Statistical Summary:\")\n",
        "df.describe(include='all')\n"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# üî¢ Dataset Dimensions\n",
        "rows, cols = df.shape\n",
        "print(f\"üìÑ Number of Rows: {rows}\")\n",
        "print(f\"üìä Number of Columns: {cols}\")\n"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚ÑπÔ∏è Dataset Info\n",
        "df.info()\n"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# üîÅ Check for duplicate rows\n",
        "duplicate_count = df.duplicated().sum()\n",
        "print(f\"Number of duplicate rows: {duplicate_count}\")\n"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# üîç Check for missing/null values in each column\n",
        "missing_values = df.isnull().sum()\n",
        "print(\"Missing values in each column:\\n\", missing_values)\n"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# üîπ Heatmap of missing values\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.heatmap(df.isnull(), cbar=False, cmap='viridis')\n",
        "plt.title('Heatmap of Missing Values')\n",
        "plt.show()\n",
        "\n",
        "# üîπ Bar plot of missing value counts\n",
        "missing = df.isnull().sum()\n",
        "missing = missing[missing > 0].sort_values(ascending=False)\n",
        "\n",
        "if not missing.empty:\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    sns.barplot(x=missing.values, y=missing.index, palette='Reds')\n",
        "    plt.title('Count of Missing Values per Column')\n",
        "    plt.xlabel('Number of Missing Values')\n",
        "    plt.ylabel('Column')\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"‚úÖ No missing values found in the dataset.\")\n"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "### üõí **Dataset Overview**\n",
        "\n",
        "The dataset is an **E-commerce transaction dataset** containing records of customer purchases. It provides detailed information about products, customers, and transactions, which we used for **customer segmentation** and **product recommendations**.\n",
        "\n",
        "---\n",
        "\n",
        "### üìå **Key Findings from Data Exploration**\n",
        "\n",
        "1. **Dataset Structure**\n",
        "\n",
        "   * **Rows:** Multiple transaction records (e.g., 500K+ rows in a typical online retail dataset).\n",
        "   * **Columns:** 8 main attributes:\n",
        "\n",
        "     * `InvoiceNo`: Transaction ID\n",
        "     * `StockCode`: Product Code\n",
        "     * `Description`: Product Name\n",
        "     * `Quantity`: Number of units purchased\n",
        "     * `InvoiceDate`: Date and time of purchase\n",
        "     * `UnitPrice`: Price per unit\n",
        "     * `CustomerID`: Unique customer identifier\n",
        "     * `Country`: Country of the customer\n",
        "\n",
        "---\n",
        "\n",
        "2. **Data Quality**\n",
        "\n",
        "   * **Missing Values:** Found mainly in the `CustomerID` column, which we handled by removing or imputing where necessary.\n",
        "   * **Duplicates:** Some duplicate records were detected and removed to ensure data integrity.\n",
        "   * **Invalid Records:**\n",
        "\n",
        "     * Cancelled invoices (identified by `InvoiceNo` starting with \"C\").\n",
        "     * Negative or zero quantities and prices were removed for accuracy.\n",
        "\n",
        "---\n",
        "\n",
        "3. **Data Characteristics**\n",
        "\n",
        "   * **Time Period:** Covers transactions between **2022‚Äì2023**.\n",
        "   * **Geographic Distribution:** Multiple countries, but a majority of customers belong to a few key markets (e.g., UK).\n",
        "   * **Top Products:** Identified best-selling products during exploratory analysis.\n",
        "   * **Purchase Trends:** Transactions showed seasonal and time-based variations.\n",
        "\n",
        "---\n",
        "\n",
        "4. **Customer Behavior Insights**\n",
        "\n",
        "   * **RFM Analysis:**\n",
        "\n",
        "     * Recency: Some customers purchased very recently, while others have been inactive for a long time.\n",
        "     * Frequency: A small percentage of customers make frequent purchases, while many purchase only occasionally.\n",
        "     * Monetary: Spending distribution is skewed‚Äîfew high-value customers contribute significantly to revenue.\n",
        "\n",
        "---\n",
        "\n",
        "5. **Business Relevance**\n",
        "\n",
        "   * The dataset is suitable for:\n",
        "     ‚úÖ Customer segmentation using clustering (RFM-based).\n",
        "     ‚úÖ Building a collaborative filtering recommendation system.\n",
        "     ‚úÖ Identifying high-value, regular, and at-risk customers.\n",
        "     ‚úÖ Supporting marketing strategies like retention programs and targeted campaigns.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display column names\n",
        "print(\"Dataset Columns:\\n\")\n",
        "print(df.columns.tolist())\n"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "# Display basic statistical summary of numerical features\n",
        "df.describe()\n"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "# Count unique values in each column\n",
        "df.nunique()\n"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code to make your dataset analysis ready.\n",
        "# üîß Make Dataset Analysis-Ready\n",
        "\n",
        "# 1. Remove rows with missing CustomerID\n",
        "df = df.dropna(subset=['CustomerID'])\n",
        "\n",
        "# 2. Remove cancelled transactions (InvoiceNo starting with 'C')\n",
        "df = df[~df['InvoiceNo'].astype(str).str.startswith('C')]\n",
        "\n",
        "# 3. Remove rows with negative or zero Quantity and UnitPrice\n",
        "df = df[(df['Quantity'] > 0) & (df['UnitPrice'] > 0)]\n",
        "\n",
        "# 4. Convert InvoiceDate to datetime\n",
        "df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'])\n",
        "\n",
        "# 5. Create a 'TotalPrice' column\n",
        "df['TotalPrice'] = df['Quantity'] * df['UnitPrice']\n",
        "\n",
        "# 6. Convert CustomerID to string (optional but helpful for merging/grouping)\n",
        "df['CustomerID'] = df['CustomerID'].astype(str)\n",
        "\n",
        "# 7. Reset index after filtering\n",
        "df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# 8. Strip whitespace in Description column (if needed)\n",
        "df['Description'] = df['Description'].str.strip()\n",
        "\n",
        "# ‚úÖ Dataset is now ready for analysis!\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Filter out United Kingdom to analyze other countries\n",
        "country_sales = df[df['Country'] != 'United Kingdom']['Country'].value_counts().head(10)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.barplot(x=country_sales.values, y=country_sales.index, palette='viridis')\n",
        "plt.title('Top 10 Countries by Number of Transactions (Excluding UK)', fontsize=14)\n",
        "plt.xlabel('Number of Transactions')\n",
        "plt.ylabel('Country')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We chose a horizontal bar chart because it clearly visualizes categorical data (countries) against a quantitative metric (number of transactions). A bar chart provides immediate visibility into which countries contribute the most to transaction volume outside the UK, which dominates the dataset."
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The top contributing countries (excluding the UK) include Netherlands, Germany, France, EIRE (Ireland), and Spain.\n",
        "\n",
        "These countries have relatively high transaction volumes, indicating potential strong customer bases.\n",
        "\n",
        "Several countries with low transaction counts may represent untapped or underperforming markets.\n",
        "\n"
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positive Impacts:\n",
        "Market Prioritization: Businesses can focus their international marketing and logistics efforts on top countries like the Netherlands and Germany.\n",
        "\n",
        "Localized Campaigns: Creating targeted offers in high-performing countries can increase customer retention and sales.\n",
        "\n",
        "Expansion Opportunities: Observing medium-tier countries like Belgium or Finland may suggest where small improvements in user experience or pricing could increase conversions.\n",
        "\n",
        "Negative Growth Signals (if any):\n",
        "Countries with very low transaction counts despite population/infrastructure (e.g., Italy or Sweden, if they appear low) may indicate:\n",
        "\n",
        "Weak brand presence or poor localization\n",
        "\n",
        "Potential technical barriers in the checkout process\n",
        "\n",
        "Limited product-market fit in those regions"
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 visualization code\n",
        "# Top 10 products by quantity sold\n",
        "top_products = df.groupby('Description')['Quantity'].sum().sort_values(ascending=False).head(10)\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.barplot(x=top_products.values, y=top_products.index, palette='mako')\n",
        "plt.title('Top 10 Selling Products by Quantity', fontsize=14)\n",
        "plt.xlabel('Total Quantity Sold')\n",
        "plt.ylabel('Product Description')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A bar chart is ideal for ranking categories‚Äîin this case, product descriptions‚Äîbased on the total quantity sold. This helps easily identify high-performing products that drive volume.\n",
        "\n"
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Certain products (e.g., ‚ÄúWHITE HANGING HEART T-LIGHT HOLDER‚Äù or ‚ÄúREGENCY CAKESTAND 3 TIER‚Äù) might be consistently appearing at the top.\n",
        "\n",
        "These products have strong appeal, indicating popularity, gift-ability, or seasonality.\n",
        "\n",
        "Some products are bulk-purchased, either for reselling or promotional usage."
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "üìà Positive Impacts:\n",
        "Inventory Management: Businesses can optimize inventory levels by focusing on stocking top-selling items, minimizing stockouts and overstocking.\n",
        "\n",
        "Sales Strategy: High-selling products can be bundled with slower-moving items to boost overall sales.\n",
        "\n",
        "Marketing Focus: These products are ideal candidates for featured placements, email campaigns, or upselling strategies.\n",
        "\n",
        "‚ö†Ô∏è Potential Negative Indicators:\n",
        "Over-reliance on a small set of products may increase business risk if supply chains break or customer trends shift.\n",
        "\n",
        "High-volume products with low profit margins could hurt long-term profitability if not analyzed alongside revenue metrics.\n",
        "\n"
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert InvoiceDate to datetime if not already\n",
        "df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'])\n",
        "\n",
        "# Create Year-Month column as string\n",
        "df['YearMonth'] = df['InvoiceDate'].dt.to_period('M').astype(str)\n",
        "\n",
        "# Calculate Revenue\n",
        "df['Revenue'] = df['Quantity'] * df['UnitPrice']\n",
        "\n",
        "# Group by YearMonth and sum Revenue\n",
        "monthly_revenue = df.groupby('YearMonth')['Revenue'].sum().reset_index()\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.lineplot(data=monthly_revenue, x='YearMonth', y='Revenue', marker='o', linewidth=2)\n",
        "plt.xticks(rotation=45)\n",
        "plt.title('Monthly Revenue Trend', fontsize=14)\n",
        "plt.xlabel('Year-Month')\n",
        "plt.ylabel('Revenue')\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A line chart is ideal to track temporal trends. It shows how revenue fluctuates month by month and helps detect patterns, seasonality, or anomalies. It‚Äôs essential for forecasting and planning."
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Peak revenue months can be identified (e.g., around holidays or festive seasons).\n",
        "\n",
        "Revenue dips might align with off-seasons, stockout issues, or macroeconomic factors.\n",
        "\n",
        "Consistent growth or decline patterns can indicate the brand's market penetration or loss of traction.\n",
        "\n"
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚úÖ Positive Impacts:\n",
        "Forecasting Demand: Seasonal trends help in forecasting sales, staffing, and marketing spend.\n",
        "\n",
        "Campaign Timing: Businesses can time promotions or new launches during revenue-peak months.\n",
        "\n",
        "Cash Flow Management: Anticipating low-revenue months helps in budgeting and reserve planning.\n",
        "\n",
        "‚ö†Ô∏è Possible Red Flags:\n",
        "Unexplained dips in revenue may indicate product unavailability, poor promotions, or competition.\n",
        "\n",
        "Irregular spikes might suggest data quality issues or one-off bulk orders skewing the trend."
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 visualization code\n",
        "# RFM Calculations\n",
        "import datetime as dt\n",
        "\n",
        "# Set reference date as one day after the last invoice date\n",
        "reference_date = df['InvoiceDate'].max() + pd.Timedelta(days=1)\n",
        "\n",
        "rfm_df = df.groupby('CustomerID').agg({\n",
        "    'InvoiceDate': lambda x: (reference_date - x.max()).days,  # Recency\n",
        "    'InvoiceNo': 'nunique',                                     # Frequency\n",
        "    'Revenue': 'sum'                                            # Monetary\n",
        "}).reset_index()\n",
        "\n",
        "rfm_df.columns = ['CustomerID', 'Recency', 'Frequency', 'Monetary']\n",
        "\n",
        "# Plot RFM Distributions\n",
        "plt.figure(figsize=(18, 5))\n",
        "\n",
        "plt.subplot(1, 3, 1)\n",
        "sns.histplot(rfm_df['Recency'], bins=30, kde=True, color='skyblue')\n",
        "plt.title('Recency Distribution')\n",
        "plt.xlabel('Days Since Last Purchase')\n",
        "\n",
        "plt.subplot(1, 3, 2)\n",
        "sns.histplot(rfm_df['Frequency'], bins=30, kde=True, color='lightgreen')\n",
        "plt.title('Frequency Distribution')\n",
        "plt.xlabel('Number of Purchases')\n",
        "\n",
        "plt.subplot(1, 3, 3)\n",
        "sns.histplot(rfm_df['Monetary'], bins=30, kde=True, color='salmon')\n",
        "plt.title('Monetary Distribution')\n",
        "plt.xlabel('Total Spend')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Understanding how Recency, Frequency, and Monetary values are distributed across customers is crucial before clustering. Histograms with KDE help spot skewness, outliers, and the overall structure of each RFM component."
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recency: Many customers haven't purchased recently ‚Äî suggesting potential churn.\n",
        "\n",
        "Frequency: A large group made only a few purchases ‚Äî typical in e-commerce, indicating a long tail of infrequent buyers.\n",
        "\n",
        "Monetary: Spending is highly skewed ‚Äî few customers contribute to the majority of revenue (Pareto principle).\n",
        "\n"
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚úÖ Positive Impacts:\n",
        "Marketing Optimization: High Recency, Low Frequency customers can be re-engaged through win-back campaigns.\n",
        "\n",
        "Revenue Focus: High-Monetary customers can be targeted with loyalty programs.\n",
        "\n",
        "Personalization: Understanding purchasing behavior aids in personalization and recommendation strategies.\n",
        "\n",
        "‚ö†Ô∏è Negative Insight:\n",
        "Over-dependence on a small segment (high spenders) is risky. A drop in their activity could significantly impact revenue."
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 visualization code\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "# Standardize RFM values\n",
        "scaler = StandardScaler()\n",
        "rfm_scaled = scaler.fit_transform(rfm_df[['Recency', 'Frequency', 'Monetary']])\n",
        "\n",
        "# Compute WCSS for different k values\n",
        "wcss = []\n",
        "silhouette_scores = []\n",
        "K = range(2, 11)\n",
        "\n",
        "for k in K:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "    kmeans.fit(rfm_scaled)\n",
        "    wcss.append(kmeans.inertia_)\n",
        "    silhouette_scores.append(silhouette_score(rfm_scaled, kmeans.labels_))\n",
        "\n",
        "# Plot Elbow Curve and Silhouette Score\n",
        "fig, ax = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Elbow Curve\n",
        "ax[0].plot(K, wcss, 'bo-')\n",
        "ax[0].set_title('Elbow Curve')\n",
        "ax[0].set_xlabel('Number of Clusters')\n",
        "ax[0].set_ylabel('WCSS')\n",
        "\n",
        "# Silhouette Score Plot\n",
        "ax[1].plot(K, silhouette_scores, 'go-')\n",
        "ax[1].set_title('Silhouette Score for k')\n",
        "ax[1].set_xlabel('Number of Clusters')\n",
        "ax[1].set_ylabel('Silhouette Score')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Elbow Method helps in selecting the optimal number of clusters (k) by visualizing the point where the reduction in WCSS starts to diminish ‚Äî forming an \"elbow\".\n",
        "\n",
        "The Silhouette Score provides an additional metric to evaluate how well-separated the clusters are.\n",
        "\n"
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The elbow in the WCSS plot likely appears around k = 4 or 5, suggesting a natural segmentation into 4 or 5 customer types.\n",
        "\n",
        "Silhouette scores help validate which k gives the best separation. Typically, higher silhouette score = better clustering."
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positive Impacts:\n",
        "Choosing the right number of clusters improves the accuracy of customer segmentation.\n",
        "\n",
        "Leads to better-targeted marketing, retention strategies, and personalized product recommendations.\n",
        "\n",
        "Prevents overfitting or underfitting segmentation logic.\n",
        "\n",
        "‚ö†Ô∏è Risk if misused:\n",
        "Choosing the wrong k (e.g., too few or too many clusters) might lead to misleading segmentation, impacting campaign ROI or misallocating resources.\n",
        "\n"
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### ‚úÖ **Step: Define Three Hypothetical Statements (Null & Alternate Hypotheses)**\n",
        "\n",
        "Here are three meaningful and business-relevant hypotheses based on your dataset and charts so far:\n",
        "\n",
        "---\n",
        "\n",
        "### üîπ **Hypothesis 1:**\n",
        "\n",
        "**Statement:** *\"There is a significant difference in average spending (Monetary) between customers from the United Kingdom and other countries.\"*\n",
        "\n",
        "* **Null Hypothesis (H‚ÇÄ):** There is **no significant difference** in average Monetary value between UK and non-UK customers.\n",
        "* **Alternate Hypothesis (H‚ÇÅ):** There **is a significant difference** in average Monetary value between UK and non-UK customers.\n",
        "\n",
        "---\n",
        "\n",
        "### üîπ **Hypothesis 2:**\n",
        "\n",
        "**Statement:** *\"High-frequency customers tend to spend more than low-frequency customers.\"*\n",
        "\n",
        "* **Null Hypothesis (H‚ÇÄ):** There is **no significant difference** in Monetary value between high and low-frequency customers.\n",
        "* **Alternate Hypothesis (H‚ÇÅ):** High-frequency customers **spend significantly more** than low-frequency customers.\n",
        "\n",
        "---\n",
        "\n",
        "### üîπ **Hypothesis 3:**\n",
        "\n",
        "**Statement:** *\"The average quantity of products purchased varies significantly across different countries.\"*\n",
        "\n",
        "* **Null Hypothesis (H‚ÇÄ):** The average quantity purchased **is the same across countries**.\n",
        "* **Alternate Hypothesis (H‚ÇÅ):** The average quantity purchased **varies significantly across countries**.\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are investigating whether average customer spending (Monetary) is significantly different between UK and non-UK customers.\n",
        "\n",
        "Null Hypothesis (H‚ÇÄ): There is no significant difference in average Monetary value between UK and non-UK customers.\n",
        "\n",
        "Alternate Hypothesis (H‚ÇÅ): There is a significant difference in average Monetary value between UK and non-UK customers."
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from scipy.stats import ttest_ind\n",
        "\n",
        "# Calculate Monetary value for each transaction\n",
        "df['Revenue'] = df['Quantity'] * df['UnitPrice']\n",
        "\n",
        "# Calculate total Monetary value per customer\n",
        "df_monetary = df.groupby('CustomerID')['Revenue'].sum().reset_index(name='Monetary')\n",
        "\n",
        "# Extract unique CustomerID and Country mapping\n",
        "df_customer_country = df[['CustomerID', 'Country']].drop_duplicates()\n",
        "\n",
        "# Merge monetary and country info\n",
        "df_combined = pd.merge(df_monetary, df_customer_country, on='CustomerID')\n",
        "\n",
        "# Filter UK and Non-UK customers\n",
        "uk_customers = df_combined[df_combined['Country'] == 'United Kingdom']['Monetary']\n",
        "non_uk_customers = df_combined[df_combined['Country'] != 'United Kingdom']['Monetary']\n",
        "\n",
        "# Perform Welch‚Äôs t-test (does not assume equal variance)\n",
        "t_stat, p_value = ttest_ind(uk_customers, non_uk_customers, equal_var=False)\n",
        "\n",
        "print(f\"T-Statistic: {t_stat}\")\n",
        "print(f\"P-Value: {p_value}\")\n"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since we are comparing means of two independent groups (UK vs non-UK customers), we will use the Independent Two-Sample t-test (Welch‚Äôs t-test if variances are unequal)."
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The independent t-test (specifically Welch‚Äôs version) is ideal here because:\n",
        "\n",
        "We are comparing two independent groups\n",
        "\n",
        "The target variable is continuous\n",
        "\n",
        "The test is robust to unequal variances"
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "## 1Ô∏è‚É£ Research Hypotheses\n",
        "\n",
        "* **Null Hypothesis (H‚ÇÄ):** There is **no significant difference** in monetary value between high-frequency customers and low-frequency customers.\n",
        "* **Alternate Hypothesis (H‚ÇÅ):** High-frequency customers **spend significantly more** than low-frequency customers.\n",
        "\n",
        "---\n",
        "\n",
        "## 2Ô∏è‚É£ Statistical Test Selection\n",
        "\n",
        "* **Chosen Test:** **Independent Samples t-test (One-tailed)**\n",
        "\n",
        "---\n",
        "\n",
        "## 3Ô∏è‚É£ Why t-test?\n",
        "\n",
        "* We are comparing the **mean monetary value** between **two independent groups** (high-frequency vs. low-frequency customers).\n",
        "* The data for each group is **continuous** (monetary values) and approximately follows a normal distribution after scaling or log transformation.\n",
        "* We are testing whether one group (high-frequency) has a **greater mean** than the other.\n",
        "\n",
        "‚úÖ Hence, a **one-tailed t-test** is appropriate.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "Z4qF1tRBEFr4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import ttest_ind\n",
        "import pandas as pd\n",
        "\n",
        "# Calculate Revenue for each transaction\n",
        "df['Revenue'] = df['Quantity'] * df['UnitPrice']\n",
        "\n",
        "# Calculate Frequency and Monetary per customer\n",
        "rfm = df.groupby('CustomerID').agg({\n",
        "    'InvoiceDate': lambda x: (df['InvoiceDate'].max() - x.max()).days,  # Recency\n",
        "    'InvoiceNo': 'nunique',  # Frequency\n",
        "    'Revenue': 'sum'         # Monetary\n",
        "}).reset_index()\n",
        "\n",
        "# Rename columns\n",
        "rfm.columns = ['CustomerID', 'Recency', 'Frequency', 'Monetary']\n",
        "\n",
        "# Split customers into high-frequency and low-frequency groups\n",
        "freq_threshold = rfm['Frequency'].median()\n",
        "high_freq = rfm[rfm['Frequency'] > freq_threshold]['Monetary']\n",
        "low_freq = rfm[rfm['Frequency'] <= freq_threshold]['Monetary']\n",
        "\n",
        "# Perform one-tailed t-test\n",
        "t_stat, p_value = ttest_ind(high_freq, low_freq, alternative='greater')\n",
        "\n",
        "print(f\"T-Statistic: {t_stat}\")\n",
        "print(f\"P-Value: {p_value}\")\n",
        "\n",
        "# Conclusion\n",
        "if p_value < 0.05:\n",
        "    print(\"‚úÖ Reject H‚ÇÄ: High-frequency customers spend significantly more.\")\n",
        "else:\n",
        "    print(\"‚ùå Fail to Reject H‚ÇÄ: No significant difference in spending.\")\n"
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3"
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## 1Ô∏è‚É£ Research Hypotheses\n",
        "\n",
        "* **Null Hypothesis (H‚ÇÄ):** There is **no significant correlation** between Recency (days since last purchase) and Monetary (total spending).\n",
        "* **Alternate Hypothesis (H‚ÇÅ):** There is a **significant negative correlation** between Recency and Monetary (i.e., customers who purchased recently tend to spend more).\n",
        "\n",
        "---\n",
        "\n",
        "## 2Ô∏è‚É£ Statistical Test Selection\n",
        "\n",
        "* **Chosen Test:** **Pearson Correlation Test**\n",
        "\n",
        "---\n",
        "\n",
        "## 3Ô∏è‚É£ Why Pearson Correlation?\n",
        "\n",
        "* Both variables (**Recency** and **Monetary**) are continuous.\n",
        "* We want to measure the **strength and direction of the linear relationship** between them.\n",
        "* Pearson correlation provides both a correlation coefficient (r) and a p-value for statistical significance.\n",
        "\n",
        "‚úÖ Hence, Pearson correlation is appropriate.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "LqvA2WggEq-j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import pearsonr\n",
        "\n",
        "# Use the RFM table\n",
        "corr_coeff, p_value = pearsonr(rfm['Recency'], rfm['Monetary'])\n",
        "\n",
        "print(f\"Correlation Coefficient (r): {corr_coeff}\")\n",
        "print(f\"P-Value: {p_value}\")\n",
        "\n",
        "# Conclusion\n",
        "if p_value < 0.05 and corr_coeff < 0:\n",
        "    print(\"‚úÖ Reject H‚ÇÄ: Significant negative correlation found between Recency and Monetary.\")\n",
        "else:\n",
        "    print(\"‚ùå Fail to Reject H‚ÇÄ: No significant correlation between Recency and Monetary.\")\n"
      ],
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "# Check initial missing value count\n",
        "print(\"Missing Values (Before Imputation):\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# Drop rows with missing 'CustomerID' (as it's crucial for RFM analysis and recommendation)\n",
        "df = df.dropna(subset=['CustomerID'])\n",
        "\n",
        "# Fill missing 'Description' values with 'Unknown' (optional, depending on your use case)\n",
        "df['Description'] = df['Description'].fillna('Unknown')\n",
        "\n",
        "# Ensure CustomerID is of type string (for similarity/recommendation systems)\n",
        "df['CustomerID'] = df['CustomerID'].astype(str)\n",
        "\n",
        "# Recheck missing values after handling\n",
        "print(\"\\nMissing Values (After Imputation):\")\n",
        "print(df.isnull().sum())\n"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments\n",
        "import numpy as np\n",
        "\n",
        "# Function to detect and treat outliers using IQR method\n",
        "def handle_outliers(df, column):\n",
        "    Q1 = df[column].quantile(0.25)\n",
        "    Q3 = df[column].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "    print(f\"\\nHandling outliers for: {column}\")\n",
        "    print(f\"Lower Bound: {lower_bound}, Upper Bound: {upper_bound}\")\n",
        "    print(f\"Original shape: {df.shape}\")\n",
        "\n",
        "    # Filter out the outliers\n",
        "    df = df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]\n",
        "\n",
        "    print(f\"New shape after outlier removal: {df.shape}\")\n",
        "    return df\n",
        "\n",
        "# Apply outlier handling to 'Quantity' and 'UnitPrice'\n",
        "df = handle_outliers(df, 'Quantity')\n",
        "df = handle_outliers(df, 'UnitPrice')\n"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode your categorical columns\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Display object-type columns\n",
        "categorical_cols = df.select_dtypes(include='object').columns\n",
        "print(\"Categorical Columns:\", categorical_cols)\n"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "le = LabelEncoder()\n",
        "\n",
        "# Encode 'Country' (you can add more if needed)\n",
        "df['Country_Encoded'] = le.fit_transform(df['Country'])\n",
        "\n",
        "# Optional: drop original column if not needed\n",
        "# df.drop('Country', axis=1, inplace=True)\n",
        "\n",
        "# Preview\n",
        "df[['Country', 'Country_Encoded']].head()\n"
      ],
      "metadata": {
        "id": "6RWorQ4Mnsin"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# One-Hot Encode the 'Country' column\n",
        "df_encoded = pd.get_dummies(df, columns=['Country'], drop_first=True)\n",
        "\n",
        "# Preview\n",
        "df_encoded.head()\n"
      ],
      "metadata": {
        "id": "jCg-_bPZnv94"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Textual Data Preprocessing\n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"
      ],
      "metadata": {
        "id": "Iwf50b-R2tYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Expand Contraction"
      ],
      "metadata": {
        "id": "GMQiZwjn3iu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Expand Contraction"
      ],
      "metadata": {
        "id": "PTouz10C3oNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Lower Casing"
      ],
      "metadata": {
        "id": "WVIkgGqN3qsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lower Casing"
      ],
      "metadata": {
        "id": "88JnJ1jN3w7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Removing Punctuations"
      ],
      "metadata": {
        "id": "XkPnILGE3zoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Punctuations"
      ],
      "metadata": {
        "id": "vqbBqNaA33c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Removing URLs & Removing words and digits contain digits."
      ],
      "metadata": {
        "id": "Hlsf0x5436Go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove URLs & Remove words and digits contain digits"
      ],
      "metadata": {
        "id": "2sxKgKxu4Ip3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Removing Stopwords & Removing White spaces"
      ],
      "metadata": {
        "id": "mT9DMSJo4nBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Stopwords"
      ],
      "metadata": {
        "id": "T2LSJh154s8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove White spaces"
      ],
      "metadata": {
        "id": "EgLJGffy4vm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6. Rephrase Text"
      ],
      "metadata": {
        "id": "c49ITxTc407N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rephrase Text"
      ],
      "metadata": {
        "id": "foqY80Qu48N2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7. Tokenization"
      ],
      "metadata": {
        "id": "OeJFEK0N496M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization"
      ],
      "metadata": {
        "id": "ijx1rUOS5CUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 8. Text Normalization"
      ],
      "metadata": {
        "id": "9ExmJH0g5HBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizing Text (i.e., Stemming, Lemmatization etc.)"
      ],
      "metadata": {
        "id": "AIJ1a-Zc5PY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text normalization technique have you used and why?"
      ],
      "metadata": {
        "id": "cJNqERVU536h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "Z9jKVxE06BC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 9. Part of speech tagging"
      ],
      "metadata": {
        "id": "k5UmGsbsOxih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# POS Taging"
      ],
      "metadata": {
        "id": "btT3ZJBAO6Ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 10. Text Vectorization"
      ],
      "metadata": {
        "id": "T0VqWOYE6DLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorizing Text"
      ],
      "metadata": {
        "id": "yBRtdhth6JDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text vectorization technique have you used and why?"
      ],
      "metadata": {
        "id": "qBMux9mC6MCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "su2EnbCh6UKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manipulate Features to minimize feature correlation and create new features\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Select numeric columns\n",
        "numeric_df = df.select_dtypes(include=['float64', 'int64'])\n",
        "\n",
        "# Plot heatmap to check correlations\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.heatmap(numeric_df.corr(), annot=True, cmap='coolwarm')\n",
        "plt.title('Correlation Matrix of Numeric Features')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: If 'Quantity' and 'TotalSpend' (or similar) are highly correlated:\n",
        "df.drop(columns=['Quantity'], inplace=True)  # only if redundant\n"
      ],
      "metadata": {
        "id": "kCgwy-WspIGA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['TotalSpend'] = df['Quantity'] * df['UnitPrice']\n"
      ],
      "metadata": {
        "id": "sNzkPPKkpK_s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'])\n",
        "df['PurchaseHour'] = df['InvoiceDate'].dt.hour\n"
      ],
      "metadata": {
        "id": "G--MhpnApYgx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['PurchaseDay'] = df['InvoiceDate'].dt.dayofweek  # Monday=0, Sunday=6\n"
      ],
      "metadata": {
        "id": "p1hl3r-1pbvQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['IsWeekend'] = df['PurchaseDay'].apply(lambda x: 1 if x >= 5 else 0)\n"
      ],
      "metadata": {
        "id": "wOp8VbHxpfru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: If using 'Country' for modeling\n",
        "df = pd.get_dummies(df, columns=['Country'], drop_first=True)\n"
      ],
      "metadata": {
        "id": "MWCC6XiWpjno"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preview of engineered features\n",
        "df[['CustomerID', 'TotalSpend', 'PurchaseHour', 'PurchaseDay', 'IsWeekend']].head()\n"
      ],
      "metadata": {
        "id": "_z7-X9OHpn6w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select your features wisely to avoid overfitting\n",
        "# Drop non-informative columns (identifiers, text descriptions, invoice number)\n",
        "df.drop(['InvoiceNo', 'StockCode', 'Description'], axis=1, inplace=True)\n"
      ],
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_selection import VarianceThreshold\n",
        "\n",
        "# Only numeric features\n",
        "num_df = df.select_dtypes(include=['int64', 'float64'])\n",
        "\n",
        "selector = VarianceThreshold(threshold=0.01)\n",
        "selector.fit(num_df)\n",
        "\n",
        "# Keep only high variance columns\n",
        "high_variance_cols = num_df.columns[selector.get_support()]\n",
        "df = df[high_variance_cols]\n"
      ],
      "metadata": {
        "id": "M-DahJr6sA4t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.heatmap(df.corr(), annot=True, cmap='coolwarm')\n",
        "plt.title(\"Correlation Matrix After Variance Filtering\")\n",
        "plt.show()\n",
        "\n",
        "# Drop one of any highly correlated pairs manually\n",
        "# Example: If 'Quantity' and 'TotalSpend' are correlated\n",
        "# df.drop(['Quantity'], axis=1, inplace=True)\n"
      ],
      "metadata": {
        "id": "iwS61UzbsDr0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose your final features manually or automatically based on importance\n",
        "selected_features = ['Recency', 'Frequency', 'Monetary', 'TotalSpend', 'PurchaseHour', 'IsWeekend']\n",
        "df_final = df[selected_features]\n"
      ],
      "metadata": {
        "id": "KTrIkcmKsLxW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which all features you found important and why?"
      ],
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "fGgaEstsBnaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform Your data\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Select RFM features or final features for modeling\n",
        "features = ['Recency', 'Frequency', 'Monetary']\n",
        "\n",
        "# Apply StandardScaler\n",
        "scaler = StandardScaler()\n",
        "scaled_features = scaler.fit_transform(df[features])\n",
        "\n",
        "# Convert back to DataFrame for ease of use\n",
        "df_scaled = pd.DataFrame(scaled_features, columns=features)\n",
        "\n",
        "# Check transformed data\n",
        "df_scaled.head()\n"
      ],
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling your data\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Selecting RFM features for scaling\n",
        "rfm_features = ['Recency', 'Frequency', 'Monetary']\n",
        "\n",
        "# Initialize StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit and transform the RFM data\n",
        "scaled_rfm = scaler.fit_transform(df[rfm_features])\n",
        "\n",
        "# Convert scaled data back into a DataFrame\n",
        "df_scaled_rfm = pd.DataFrame(scaled_rfm, columns=rfm_features)\n",
        "\n",
        "# Display first few rows\n",
        "df_scaled_rfm.head()\n"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?"
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the RFM‚Äëbased customer‚Äêsegmentation task, dimensionality reduction is not strictly necessary because we are already working with only three features ‚Äî‚ÄØRecency, Frequency, Monetary (R,‚ÄØF,‚ÄØM). Three numeric dimensions are perfectly manageable for K‚ÄëMeans and allow straightforward interpretation of clusters."
      ],
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DImensionality Reduction (If needed)"
      ],
      "metadata": {
        "id": "kQfvxBBHDvCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"
      ],
      "metadata": {
        "id": "T5CmagL3EC8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "ZKr75IDuEM7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely.\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Example: Suppose your final feature set is stored in X and labels (if any) in y.\n",
        "# If you're doing supervised learning (e.g., classification or regression), you'll have both X and y.\n",
        "# For clustering (unsupervised), only X is needed.\n",
        "\n",
        "# Example X and y (modify as per your actual variable names)\n",
        "# X = df[['Recency', 'Frequency', 'Monetary']]  # already scaled or encoded\n",
        "# y = df['Target_Label']  # if you're doing classification (optional)\n",
        "\n",
        "# For supervised ML task (e.g., classification or regression)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.2,      # 20% test set\n",
        "    random_state=42,    # for reproducibility\n",
        "    stratify=y if 'y' in locals() else None  # for classification tasks\n",
        ")\n",
        "\n",
        "print(\"Training Set Shape:\", X_train.shape)\n",
        "print(\"Testing Set Shape:\", X_test.shape)\n"
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why?"
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "P1XJ9OREExlT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ],
      "metadata": {
        "id": "VFOzZv6IFROw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "GeKDIv7pFgcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Imbalanced Dataset (If needed)\n",
        "# Checking value counts of target variable (assuming it's classification)\n",
        "# For customer segmentation, assume you already have clusters labeled\n",
        "df['Cluster'].value_counts().plot(kind='bar', color='skyblue')\n",
        "plt.title('Class Distribution')\n",
        "plt.xlabel('Cluster')\n",
        "plt.ylabel('Number of Customers')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "nQsRhhZLFiDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Assuming X and y are already defined\n",
        "smote = SMOTE(random_state=42)\n",
        "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
        "\n",
        "print(\"Before Resampling:\", y.value_counts())\n",
        "print(\"After Resampling:\", pd.Series(y_resampled).value_counts())\n"
      ],
      "metadata": {
        "id": "_MtjgFD1PI3g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"
      ],
      "metadata": {
        "id": "TIqpNgepFxVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "qbet1HwdGDTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1Ô∏è‚É£ Calculate Recency, Frequency, Monetary\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "\n",
        "# Ensure InvoiceDate is datetime\n",
        "df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'])\n",
        "\n",
        "# Reference date (max date in dataset)\n",
        "ref_date = df['InvoiceDate'].max()\n",
        "\n",
        "rfm = df.groupby('CustomerID').agg({\n",
        "    'InvoiceDate': lambda x: (ref_date - x.max()).days,  # Recency\n",
        "    'InvoiceNo': 'nunique',                              # Frequency\n",
        "    'Revenue': 'sum'                                     # Monetary (ensure Revenue = Quantity * UnitPrice)\n",
        "}).reset_index()\n",
        "\n",
        "rfm.columns = ['CustomerID', 'Recency', 'Frequency', 'Monetary']\n",
        "\n",
        "# 2Ô∏è‚É£ Apply KMeans for clustering\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "scaler = StandardScaler()\n",
        "rfm_scaled = scaler.fit_transform(rfm[['Recency', 'Frequency', 'Monetary']])\n",
        "\n",
        "kmeans = KMeans(n_clusters=4, random_state=42)\n",
        "rfm['Cluster'] = kmeans.fit_predict(rfm_scaled)\n",
        "\n",
        "# 3Ô∏è‚É£ Split data into training/testing\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = rfm[['Recency', 'Frequency', 'Monetary']]\n",
        "y = rfm['Cluster']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "Jyg8ta4Ji3J1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "\n",
        "rf_model = RandomForestClassifier(random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = rf_model.predict(X_test)\n",
        "\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
        "print(\"Accuracy Score:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=rf_model.classes_,\n",
        "            yticklabels=rf_model.classes_)\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "import pandas as pd\n",
        "\n",
        "# Get classification report as a dataframe\n",
        "report = classification_report(y_test, y_pred, output_dict=True)\n",
        "report_df = pd.DataFrame(report).transpose()\n",
        "\n",
        "# Plot precision, recall, f1-score\n",
        "report_df.iloc[:-1, :-1].plot(kind='bar', figsize=(10, 6), colormap='Paired')\n",
        "plt.title(\"Classification Metrics (Precision, Recall, F1-score)\")\n",
        "plt.ylabel(\"Score\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(axis='y')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "w36h5NkVRrhD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model\n",
        "\n",
        "# Import required libraries\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "\n",
        "# Define parameter grid for GridSearchCV\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [5, 10, 15, None],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'max_features': ['sqrt', 'log2']\n",
        "}\n",
        "\n",
        "# Initialize model\n",
        "rf_model = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Apply GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid,\n",
        "                           cv=5, scoring='accuracy', n_jobs=-1, verbose=1)\n",
        "\n",
        "# Fit the model\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best estimator after Grid Search\n",
        "best_rf_model = grid_search.best_estimator_\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = best_rf_model.predict(X_test)\n",
        "\n",
        "# Evaluation metrics\n",
        "print(\"Best Parameters Found:\\n\", grid_search.best_params_)\n",
        "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
        "print(\"Accuracy Score:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### ‚úÖ **What is GridSearchCV?**\n",
        "\n",
        "GridSearchCV is a brute-force hyperparameter tuning method in which the algorithm tests **all possible combinations** of hyperparameters specified in a parameter grid. It then selects the combination that gives the **best cross-validation performance**.\n",
        "\n",
        "---\n",
        "\n",
        "### üìå **Why GridSearchCV was used in this project?**\n",
        "\n",
        "1. **Exhaustive Search:**\n",
        "   Since the dataset size is manageable, GridSearchCV can systematically test all combinations and ensure the best parameters are found.\n",
        "\n",
        "2. **High Accuracy for Tabular Data:**\n",
        "   Random Forests are sensitive to parameters like `n_estimators`, `max_depth`, and `min_samples_split`. GridSearchCV ensures the best values for these hyperparameters are selected.\n",
        "\n",
        "3. **Cross-Validation Integration:**\n",
        "   GridSearchCV performs **k-fold cross-validation** during the search, reducing the risk of overfitting and making the model more generalizable.\n",
        "\n",
        "4. **Interpretability:**\n",
        "   The output includes the **best parameters** (`best_params_`) and the **best estimator**, which simplifies model evaluation and deployment.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ‚úÖ Improvement After Hyperparameter Tuning\n",
        "\n",
        "After applying **GridSearchCV** on the Random Forest Classifier, there was a **slight but measurable improvement** in the model‚Äôs performance:\n",
        "\n",
        "| Metric                | Before Tuning | After Tuning |\n",
        "| --------------------- | ------------- | ------------ |\n",
        "| **Accuracy**          | 0.996         | **0.9977**   |\n",
        "| **Precision (Macro)** | 0.98 (approx) | **0.99**     |\n",
        "| **Recall (Macro)**    | 0.87 (approx) | **0.88**     |\n",
        "| **F1-Score (Macro)**  | 0.90 (approx) | **0.91**     |\n",
        "\n",
        "---\n",
        "\n",
        "### Key Insights:\n",
        "\n",
        "* The tuned model achieved a **higher recall for rare classes** (especially class `2`, which improved from lower values in earlier training).\n",
        "* Accuracy improvement from **99.6% to 99.77%** shows better generalization without overfitting.\n",
        "* The **confusion matrix** shows fewer misclassifications across all clusters.\n",
        "* The **macro average metrics** improved slightly, showing better handling of minority segments.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------------\n",
        "# üì¶ Import required libraries\n",
        "# ----------------------------------\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ----------------------------------\n",
        "# ‚úÖ Fit the Algorithm\n",
        "# ----------------------------------\n",
        "xgb_model = XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss')\n",
        "xgb_model.fit(X_train, y_train)\n",
        "\n",
        "# ----------------------------------\n",
        "# üìå Predict on the model\n",
        "# ----------------------------------\n",
        "y_pred_xgb = xgb_model.predict(X_test)\n",
        "\n",
        "# ----------------------------------\n",
        "# üìä Evaluation\n",
        "# ----------------------------------\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_xgb))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_xgb))\n",
        "print(\"Accuracy Score:\", accuracy_score(y_test, y_pred_xgb))\n",
        "\n",
        "# üìà Optional: Confusion Matrix Heatmap\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(confusion_matrix(y_test, y_pred_xgb), annot=True, fmt=\"d\", cmap=\"YlGnBu\")\n",
        "plt.title(\"Confusion Matrix - XGBoost\")\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "tqvVJ0wwSPe6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "# ----------------------------------\n",
        "# üì¶ Import required libraries\n",
        "# ----------------------------------\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ----------------------------------\n",
        "# üéØ Define parameter grid for XGBoost\n",
        "# ----------------------------------\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100],\n",
        "    'max_depth': [3, 5],\n",
        "    'learning_rate': [0.05, 0.1],\n",
        "    'subsample': [0.8, 1],\n",
        "}\n",
        "\n",
        "# ----------------------------------\n",
        "# ‚úÖ Grid Search with Cross Validation\n",
        "# ----------------------------------\n",
        "xgb = XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss')\n",
        "\n",
        "grid_search = GridSearchCV(estimator=xgb, param_grid=param_grid,\n",
        "                           cv=3, scoring='accuracy', n_jobs=-1, verbose=1)\n",
        "\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# ----------------------------------\n",
        "# üìå Predict on the best model\n",
        "# ----------------------------------\n",
        "best_xgb = grid_search.best_estimator_\n",
        "y_pred = best_xgb.predict(X_test)\n",
        "\n",
        "# ----------------------------------\n",
        "# üßæ Evaluation Metrics\n",
        "# ----------------------------------\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "prec = precision_score(y_test, y_pred, average='weighted')\n",
        "rec = recall_score(y_test, y_pred, average='weighted')\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "# ----------------------------------\n",
        "# üìä Visualization of Evaluation Metrics\n",
        "# ----------------------------------\n",
        "metrics = {'Accuracy': acc, 'Precision': prec, 'Recall': rec, 'F1 Score': f1}\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.barplot(x=list(metrics.keys()), y=list(metrics.values()), palette='Set2')\n",
        "plt.title(\"XGBoost Evaluation Metrics after Hyperparameter Tuning\")\n",
        "plt.ylabel(\"Score\")\n",
        "plt.ylim(0, 1)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "BDKtOrBQpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "YnvVTiIxBL-C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ],
      "metadata": {
        "id": "EyNgTHvd2WFk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict clusters for test data\n",
        "test_clusters = kmeans.predict(X_test)\n",
        "\n",
        "# Add predictions to test dataset\n",
        "test_results = X_test.copy()\n",
        "test_results['Cluster'] = test_clusters\n",
        "\n",
        "print(test_results['Cluster'].value_counts())\n",
        "\n",
        "# Check cluster profiles\n",
        "cluster_profiles = test_results.groupby('Cluster').mean()\n",
        "print(cluster_profiles)\n"
      ],
      "metadata": {
        "id": "H9BVxjRWF4tt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "# Save K-Means clustering model\n",
        "joblib.dump(kmeans, \"kmeans_model.pkl\")\n",
        "\n",
        "# Save Random Forest model\n",
        "joblib.dump(best_rf_model, \"random_forest_model.pkl\")\n",
        "\n",
        "print(\"‚úÖ Models saved successfully!\")\n"
      ],
      "metadata": {
        "id": "dPBe2m62qdku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import joblib\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Load saved models\n",
        "kmeans_model = joblib.load(\"kmeans_model.pkl\")\n",
        "rf_model = joblib.load(\"random_forest_model.pkl\")\n",
        "\n",
        "# Instead of CSV, load transaction data or create a mock dataset\n",
        "# You can replace this with your actual dataset later\n",
        "data = {\n",
        "    \"CustomerID\": [1, 1, 2, 2, 3],\n",
        "    \"Description\": [\"GREEN VINTAGE SPOT BEAKER\", \"BLUE VINTAGE SPOT BEAKER\",\n",
        "                    \"PINK VINTAGE SPOT BEAKER\", \"POTTING SHED CANDLE CITRONELLA\",\n",
        "                    \"PANTRY CHOPPING BOARD\"],\n",
        "    \"Quantity\": [2, 1, 3, 1, 2]\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Create Product-Product similarity matrix\n",
        "product_matrix = df.pivot_table(index='CustomerID', columns='Description', values='Quantity', fill_value=0)\n",
        "similarity_matrix = pd.DataFrame(\n",
        "    cosine_similarity(product_matrix.T),\n",
        "    index=product_matrix.columns,\n",
        "    columns=product_matrix.columns\n",
        ")\n",
        "\n",
        "# Sidebar Navigation\n",
        "st.sidebar.title(\"Navigation\")\n",
        "menu = st.sidebar.radio(\"Go to:\", [\"Home\", \"Clustering\", \"Recommendation\"])\n",
        "\n",
        "# ---------------- HOME ----------------\n",
        "if menu == \"Home\":\n",
        "    st.title(\"Shopper Spectrum Dashboard\")\n",
        "    st.write(\"Welcome to the **Customer Segmentation & Product Recommendation System**. Use the sidebar to navigate.\")\n",
        "\n",
        "# ---------------- CLUSTERING ----------------\n",
        "elif menu == \"Clustering\":\n",
        "    st.title(\"Customer Segmentation\")\n",
        "    st.write(\"Enter customer details to predict their segment:\")\n",
        "\n",
        "    recency = st.number_input(\"Recency (days)\", min_value=0, value=50)\n",
        "    frequency = st.number_input(\"Frequency (purchases)\", min_value=0, value=5)\n",
        "    monetary = st.number_input(\"Monetary (spending)\", min_value=0, value=500)\n",
        "\n",
        "    if st.button(\"Predict Cluster\"):\n",
        "        input_data = pd.DataFrame([[recency, frequency, monetary]], columns=[\"Recency\", \"Frequency\", \"Monetary\"])\n",
        "        cluster = kmeans_model.predict(input_data)[0]\n",
        "\n",
        "        segment_map = {0: \"High-Value\", 1: \"Regular\", 2: \"Occasional\", 3: \"At-Risk\"}\n",
        "        st.success(f\"Predicted Cluster: **{segment_map.get(cluster, 'Unknown')}**\")\n",
        "\n",
        "# ---------------- RECOMMENDATION ----------------\n",
        "elif menu == \"Recommendation\":\n",
        "    st.title(\"Product Recommender\")\n",
        "\n",
        "    product_name = st.text_input(\"Enter Product Name\")\n",
        "    if st.button(\"Recommend\"):\n",
        "        if product_name in similarity_matrix.columns:\n",
        "            recommendations = similarity_matrix[product_name].sort_values(ascending=False)[1:6].index\n",
        "            st.write(\"### Recommended Products:\")\n",
        "            for item in recommendations:\n",
        "                st.write(f\"- {item}\")\n",
        "        else:\n",
        "            st.error(\"‚ùå Product not found. Please try another name.\")\n"
      ],
      "metadata": {
        "id": "7CZcdVCgGHkp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8ef7866-bac3-47d1-b5b8-4bd139f73eca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TeW8g9YdwTdu",
        "outputId": "92028e60-af9f-462a-a321-83ffd7a48b8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting streamlit\n",
            "  Downloading streamlit-1.47.1-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.2.1)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.0.2)\n",
            "Requirement already satisfied: packaging<26,>=20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (25.0)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (11.3.0)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.29.5)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.32.3)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.5.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.14.1)\n",
            "Collecting watchdog<7,>=2.1.5 (from streamlit)\n",
            "  Downloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit) (3.1.45)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.74.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.14.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (4.25.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (1.48.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2025.7.14)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.8.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.26.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Downloading streamlit-1.47.1-py3-none-any.whl (9.9 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m106.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m108.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: watchdog, pydeck, streamlit\n",
            "Successfully installed pydeck-0.9.1 streamlit-1.47.1 watchdog-6.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyngrok"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1v0qPkycwVeZ",
        "outputId": "e0e18065-cedb-4488-d554-be62969896be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.2.12-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
            "Downloading pyngrok-7.2.12-py3-none-any.whl (26 kB)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.2.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "li27b398vYqh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!curl ipv4.icanhazip.com"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eFTmI2VRwlwR",
        "outputId": "9e4d8fd0-2478-453f-acd6-77e187012b26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "34.86.31.110\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run  app.py & npx localtunnel --port 8501"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0hjgHFuWwqQK",
        "outputId": "22479815-18a4-489f-a45d-5d313891815c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1G\u001b[0K‚†ô\u001b[1G\u001b[0K‚†π\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[1G\u001b[0K‚†∏\u001b[1G\u001b[0K‚†º\u001b[1G\u001b[0K‚†¥\u001b[1G\u001b[0K‚†¶\u001b[1G\u001b[0K‚†ß\u001b[1G\u001b[0K‚†á\u001b[1G\u001b[0K‚†è\u001b[1G\u001b[0K‚†ã\u001b[1G\u001b[0K‚†ô\u001b[1G\u001b[0K‚†π\u001b[1G\u001b[0K‚†∏\u001b[1G\u001b[0K‚†º\u001b[1G\u001b[0K‚†¥\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.86.31.110:8501\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[1G\u001b[0K‚†¶\u001b[1G\u001b[0K‚†ß\u001b[1G\u001b[0K‚†á\u001b[1G\u001b[0K‚†è\u001b[1G\u001b[0K‚†ã\u001b[1G\u001b[0K‚†ô\u001b[1G\u001b[0K‚†π\u001b[1G\u001b[0K\u001b[1G\u001b[0JNeed to install the following packages:\n",
            "localtunnel@2.0.2\n",
            "Ok to proceed? (y) \u001b[20Gy\n",
            "\n",
            "\u001b[1G\u001b[0K‚†ô\u001b[1G\u001b[0K‚†π\u001b[1G\u001b[0K‚†∏\u001b[1G\u001b[0K‚†º\u001b[1G\u001b[0K‚†¥\u001b[1G\u001b[0K‚†¶\u001b[1G\u001b[0K‚†ß\u001b[1G\u001b[0K‚†á\u001b[1G\u001b[0K‚†è\u001b[1G\u001b[0K‚†ã\u001b[1G\u001b[0K‚†ô\u001b[1G\u001b[0K‚†π\u001b[1G\u001b[0K‚†∏\u001b[1G\u001b[0K‚†º\u001b[1G\u001b[0K‚†¥\u001b[1G\u001b[0K‚†¶\u001b[1G\u001b[0K‚†ß\u001b[1G\u001b[0K‚†á\u001b[1G\u001b[0K‚†è\u001b[1G\u001b[0K‚†ã\u001b[1G\u001b[0K‚†ô\u001b[1G\u001b[0K‚†π\u001b[1G\u001b[0K‚†∏\u001b[1G\u001b[0K‚†º\u001b[1G\u001b[0K‚†¥\u001b[1G\u001b[0K‚†¶\u001b[1G\u001b[0K‚†ß\u001b[1G\u001b[0K‚†á\u001b[1G\u001b[0K‚†è\u001b[1G\u001b[0Kyour url is: https://forty-toes-brush.loca.lt\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2732: UserWarning: X has feature names, but KMeans was fitted without feature names\n",
            "  warnings.warn(\n",
            "\u001b[34m  Stopping...\u001b[0m\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ],
      "metadata": {
        "id": "KH5McJBi2d8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the File"
      ],
      "metadata": {
        "id": "bQIANRl32f4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"
      ],
      "metadata": {
        "id": "iW_Lq9qf2h6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the File and predict unseen data."
      ],
      "metadata": {
        "id": "oEXk9ydD2nVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## üèÜ **Conclusion**\n",
        "\n",
        "The **Shopper Spectrum: Customer Segmentation and Product Recommendations in E-Commerce** project successfully demonstrates the integration of data-driven insights, machine learning, and recommendation systems to address key business challenges in the retail and e-commerce domain.\n",
        "\n",
        "Through comprehensive **data preprocessing, exploratory data analysis (EDA), and RFM-based feature engineering**, the dataset was transformed into a structured format suitable for unsupervised learning. Applying clustering techniques like **K-Means** allowed us to segment customers into meaningful groups, such as **High-Value, Regular, Occasional, and At-Risk customers**. This segmentation can be directly utilized by marketing teams for **targeted campaigns, retention programs, and dynamic pricing strategies**, thus improving business decision-making and profitability.\n",
        "\n",
        "In addition, we developed an **item-based collaborative filtering recommendation system**, leveraging product purchase patterns to suggest the top 5 relevant products for any given input. This feature addresses the critical need for **personalized customer experience**, which is proven to drive engagement, increase cross-selling, and improve overall customer satisfaction.\n",
        "\n",
        "To ensure robust model performance, we implemented **hyperparameter tuning (GridSearchCV)** for the Random Forest classifier, improving accuracy from **99.6% to 99.7%**, further validating the model's reliability. The clustering results, combined with the recommendation engine, were integrated into a **Streamlit application** for seamless real-time interaction, enabling business users and stakeholders to access actionable insights without any technical expertise.\n",
        "\n",
        "### ‚úÖ Business Impact:\n",
        "\n",
        "* Enhanced **customer targeting** through accurate segmentation.\n",
        "* Improved **customer retention** by identifying and addressing at-risk customers.\n",
        "* Boosted **sales and engagement** using personalized recommendations.\n",
        "* Provided a scalable solution that can adapt to growing datasets in the e-commerce ecosystem.\n",
        "\n",
        "In conclusion, this project establishes a strong foundation for leveraging **machine learning in e-commerce analytics**. With future enhancements like deep learning-based recommendations, real-time data integration, and advanced visualization dashboards, this solution can evolve into a comprehensive AI-driven platform for **customer behavior analytics and product intelligence**.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}