{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "vncDsAP0Gaoa",
        "FJNUwmbgGyua",
        "mDgbUHAGgjLW",
        "Iwf50b-R2tYG",
        "WVIkgGqN3qsr",
        "XkPnILGE3zoT",
        "Hlsf0x5436Go",
        "mT9DMSJo4nBL",
        "c49ITxTc407N",
        "OeJFEK0N496M",
        "9ExmJH0g5HBk",
        "cJNqERVU536h",
        "k5UmGsbsOxih",
        "T0VqWOYE6DLQ",
        "qBMux9mC6MCf",
        "yiiVWRdJDDil",
        "VfCC591jGiD4",
        "-Kee-DAl2viO",
        "gIfDvo9L0UH2"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    - Yes Bank Stock Closing Price Prediction\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Regression\n",
        "##### **Contribution**    - Individual\n",
        "##### **Contributor**     - UNNIMAYA K\n"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This project focuses on predicting the monthly closing price of Yes Bank's stock using regression techniques based on historical data. Stock market forecasting plays a significant role in the financial sector, as it enables investors and analysts to make informed decisions. Yes Bank, a major private sector bank in India, has experienced dramatic fluctuations in its stock price over the years, especially after 2018 due to financial instability and governance issues. These fluctuations make it an ideal case study for stock price analysis and prediction using machine learning.\n",
        "\n",
        "The dataset used contains monthly records of Yes Bank's stock prices, including attributes such as Open, High, Low, and Close prices for each month. The closing price, which is the stock’s price at the end of the month, is the target variable to be predicted. The goal is to explore the relationship between the available features (Open, High, Low) and the closing price to build accurate predictive models.\n",
        "\n",
        "To begin, the data was preprocessed by converting the 'Date' column into a datetime format and sorting the dataset chronologically. No missing values or inconsistencies were found, ensuring clean input for model training. Exploratory Data Analysis (EDA) was then performed to understand the structure and behavior of the dataset. Line plots were used to visualize trends in closing prices over time, and a correlation heatmap revealed that the closing price was strongly correlated with the open, high, and low prices.\n",
        "\n",
        "After data analysis, the features 'Open', 'High', and 'Low' were selected to predict the 'Close' price. The dataset was split into training and testing sets while maintaining time order to avoid data leakage. Two regression models were then implemented: Linear Regression and Random Forest Regressor. Linear Regression, a straightforward and interpretable model, served as a baseline. The Random Forest Regressor, an ensemble model that builds multiple decision trees, was used to enhance prediction accuracy.\n",
        "\n",
        "Model performance was evaluated using standard regression metrics such as Mean Absolute Error (MAE), Root Mean Squared Error (RMSE), and R² Score. The Random Forest model outperformed Linear Regression in all metrics, showing higher prediction accuracy and robustness, particularly during periods of high volatility. Visualization of actual versus predicted closing prices confirmed the effectiveness of the Random Forest model in tracking stock price movements.\n",
        "\n",
        "To further improve the model, hyperparameter tuning was performed using GridSearchCV, which further enhanced performance by selecting optimal model parameters. The final model provided reliable predictions that closely followed the actual closing price trends.\n",
        "\n",
        "In conclusion, this project demonstrates how machine learning can be effectively used for stock price prediction. The analysis of Yes Bank's stock data using regression techniques not only achieved accurate results but also provided insights into the behavior of the stock market. The Random Forest model, in particular, proved to be a powerful tool for financial forecasting. This approach can be extended to include additional features such as trading volume or technical indicators and can be adapted to predict prices of other stocks. With further refinement, such models can serve as valuable tools for investors, analysts, and financial institutions looking to make data-driven decisions.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Provide your GitHub Link here."
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Yes Bank, a major private bank in India, has shown significant price fluctuations in recent years, making it a relevant case for analysis.This project aims to predict the monthly closing price of Yes Bank stock using historical data, specifically the Open, High, and Low prices. The objective is to build an accurate regression model that can assist investors and analysts in making informed decisions based on predicted price trends.**\n",
        "\n"
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "\n",
        "# For data manipulation and analysis\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# For data visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# To ignore warnings (for cleaner output)\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# For displaying charts inline in Jupyter notebooks\n",
        "%matplotlib inline\n",
        "\n",
        "# For machine learning models and evaluation\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "# For datetime parsing and formatting\n",
        "from datetime import datetime\n",
        "\n",
        "# Display settings for dataframes\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', 100)\n",
        "pd.set_option('display.float_format', lambda x: '%.2f' % x)\n",
        "\n",
        "print(\"✅ All necessary libraries have been successfully imported.\")\n"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "\n",
        "file_path = 'data_YesBank_StockPrices.csv'  # Make sure this file is in your working directory\n",
        "\n",
        "try:\n",
        "    # Load the CSV file into a DataFrame\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    # Display basic information about the dataset\n",
        "    print(\"✅ Dataset loaded successfully!\\n\")\n",
        "    print(\"📌 First 5 rows of the dataset:\")\n",
        "    display(df.head())\n",
        "\n",
        "    print(\"\\n📌 Dataset Summary:\")\n",
        "    df.info()\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"❌ Error: The file '{file_path}' was not found. Please check the path and try again.\")\n",
        "except pd.errors.ParserError:\n",
        "    print(f\"❌ Error: Failed to parse the file '{file_path}'. Please check if it's a valid CSV.\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ An unexpected error occurred: {e}\")\n"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Dataset First Look\n",
        "\n",
        "print(\"🔎 First 5 rows of the dataset:\")\n",
        "display(df.head())  # Shows the first few records\n",
        "\n",
        "print(\"\\n📏 Dataset Shape (Rows, Columns):\", df.shape)  # Number of rows and columns\n",
        "\n",
        "print(\"\\n🧠 Column Names:\")\n",
        "print(df.columns.tolist())  # List of column names\n",
        "\n",
        "print(\"\\n🔍 Dataset Info:\")\n",
        "df.info()  # Overview of data types and non-null counts\n",
        "\n",
        "print(\"\\n📊 Statistical Summary:\")\n",
        "display(df.describe())  # Summary stats for numerical columns\n",
        "\n",
        "print(\"\\n🧪 Checking for Missing Values:\")\n",
        "print(df.isnull().sum())  # Count of missing values per column\n",
        "\n",
        "print(\"\\n🔁 Checking for Duplicated Rows:\")\n",
        "print(f\"Total Duplicated Rows: {df.duplicated().sum()}\")\n",
        "\n",
        "# Optional: Display unique values in 'Date' column (if needed for understanding time span)\n",
        "print(\"\\n📅 Time Range Covered:\")\n",
        "if 'Date' in df.columns:\n",
        "    try:\n",
        "        df['Date'] = pd.to_datetime(df['Date'], format='%b-%y')\n",
        "        print(f\"Date Range: {df['Date'].min().strftime('%b %Y')} to {df['Date'].max().strftime('%b %Y')}\")\n",
        "    except Exception as e:\n",
        "        print(\"⚠️ Couldn't convert 'Date' column to datetime format:\", e)\n"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "print(\"\\n📏 Dataset Shape (Rows, Columns):\", df.shape)  # Number of rows and columns\n",
        "print(\"\\n🧠 Column Names:\")\n",
        "print(df.columns.tolist())  # List of column names\n"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "print(\"\\n🔍 Dataset Info:\")\n",
        "df.info()  # Overview of data types and non-null counts\n",
        "print(\"\\n📊 Statistical Summary:\")\n",
        "display(df.describe())  # Summary stats for numerical columns"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "\n",
        "# Count the total number of duplicated rows\n",
        "duplicate_count = df.duplicated().sum()\n",
        "\n",
        "# Display the result\n",
        "print(f\"🔁 Total Duplicate Rows in the Dataset: {duplicate_count}\")\n",
        "\n",
        "# Optional: Display duplicated rows (if any)\n",
        "if duplicate_count > 0:\n",
        "    print(\"\\n🧾 Sample of Duplicated Rows:\")\n",
        "    display(df[df.duplicated()])\n",
        "else:\n",
        "    print(\"✅ No duplicate rows found in the dataset.\")\n"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "# Count missing values per column\n",
        "missing_values = df.isnull().sum()\n",
        "\n",
        "# Display the count\n",
        "print(\"🧪 Missing Values Count (per column):\")\n",
        "print(missing_values)\n",
        "\n",
        "# Total missing values\n",
        "total_missing = missing_values.sum()\n",
        "print(f\"\\n🔢 Total Missing Values in Dataset: {total_missing}\")"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "# Visualize missing values using a heatmap\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.heatmap(df.isnull(), cmap=\"viridis\", cbar=False, yticklabels=False, linewidths=0.5)\n",
        "plt.title(\"🔍 Heatmap of Missing Values in Dataset\", fontsize=14)\n",
        "plt.xlabel(\"Columns\")\n",
        "plt.ylabel(\"Records\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Structure & Size:\n",
        "The dataset contains monthly stock price data of Yes Bank, with columns like Date, Open, High, Low, and Close.\n",
        "Date Format:\n",
        "The Date column is in month-year (e.g., Jan-15) format and needs to be converted to datetime for time series operations.\n",
        "\n",
        "No Categorical Columns:\n",
        "All columns are numerical or time-based. This makes it ideal for regression modeling.\n",
        "\n",
        "Duplicate Records:\n",
        "There are no duplicated rows in the dataset, ensuring data uniqueness and integrity.\n",
        "\n",
        "Missing Values:\n",
        "The dataset contains minimal or no missing values (based on analysis). If present, they are likely concentrated in a specific column or row.\n",
        "\n",
        "Correlated Features:\n",
        "Initial inspection indicates that Open, High, and Low prices are strongly correlated with Close, making them useful predictors.\n",
        "\n",
        "Use Case:\n",
        "The dataset is well-suited for building a regression model to predict the Close price of Yes Bank stock, using features such as Open, High, and Low."
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "# Display all column names\n",
        "print(\"📌 Dataset Columns:\")\n",
        "print(df.columns.tolist())\n",
        "\n",
        "# Display data types and non-null counts\n",
        "print(\"\\n🧠 Dataset Info:\")\n",
        "df.info()"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "# Descriptive statistics for numerical columns\n",
        "print(\"\\n📊 Statistical Summary of Numerical Features:\")\n",
        "display(df.describe())\n",
        "\n",
        "# Explanation of each column (Optional Markdown Cell in Notebook)\n",
        "column_explanation = {\n",
        "    'Date': 'The month and year of the stock price record.',\n",
        "    'Open': 'The price at which the stock opened for the month.',\n",
        "    'High': 'The highest price reached by the stock during the month.',\n",
        "    'Low': 'The lowest price reached during the month.',\n",
        "    'Close': 'The final trading price of the stock for the month (Target Variable).'\n",
        "}\n",
        "\n",
        "print(\"\\n📘 Feature Descriptions:\")\n",
        "for col, desc in column_explanation.items():\n",
        "    print(f\"- {col}: {desc}\")"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Date\n",
        "\n",
        "Type: Object (converted to datetime)\n",
        "\n",
        "Description: Represents the month and year of each stock price record. This is used for chronological sorting and time-based analysis.\n",
        "\n",
        "Role: Used as a temporal index, not a direct feature for the model.\n",
        "\n",
        "Open\n",
        "\n",
        "Type: Float\n",
        "\n",
        "Description: The stock's opening price at the beginning of the month.\n",
        "\n",
        "Role: Predictor feature – indicates the market's starting sentiment for that month.\n",
        "\n",
        "High\n",
        "\n",
        "Type: Float\n",
        "\n",
        "Description: The highest price reached by the stock during the month.\n",
        "\n",
        "Role: Predictor feature – useful for assessing the month's price range.\n",
        "\n",
        "Low\n",
        "\n",
        "Type: Float\n",
        "\n",
        "Description: The lowest price the stock dropped to in that month.\n",
        "\n",
        "Role: Predictor feature – useful for understanding downside risk during the period.\n",
        "\n",
        "Close\n",
        "\n",
        "Type: Float\n",
        "\n",
        "Description: The final trading price of the stock at the end of the month.\n",
        "\n",
        "Role: Target variable – this is the value we are aiming to predict using regression models."
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "print(\"🔍 Unique Values Count for Each Column:\\n\")\n",
        "for column in df.columns:\n",
        "    unique_vals = df[column].nunique()\n",
        "    print(f\"- {column}: {unique_vals} unique values\")\n",
        "\n",
        "# Optional: Show the actual unique values for non-numerical or low-cardinality columns\n",
        "print(\"\\n📅 Sample Unique Values in 'Date' Column:\")\n",
        "if 'Date' in df.columns:\n",
        "    print(df['Date'].unique()[:10])  # Show only first 10 unique entries for readability"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code to make your dataset analysis ready.\n",
        "try:\n",
        "    # ✅ Convert 'Date' column to datetime format\n",
        "    df['Date'] = pd.to_datetime(df['Date'], format='%b-%y')\n",
        "\n",
        "    # ✅ Sort data chronologically\n",
        "    df = df.sort_values(by='Date').reset_index(drop=True)\n",
        "\n",
        "    # ✅ Check for and drop any duplicate rows (if found)\n",
        "    initial_shape = df.shape\n",
        "    df.drop_duplicates(inplace=True)\n",
        "    final_shape = df.shape\n",
        "\n",
        "    # ✅ Recheck data types and final structure\n",
        "    print(\"📅 Date column converted to datetime.\")\n",
        "    print(\"🔃 Dataset sorted by date.\")\n",
        "    if initial_shape != final_shape:\n",
        "        print(f\"🧹 Removed {initial_shape[0] - final_shape[0]} duplicate rows.\")\n",
        "    else:\n",
        "        print(\"✅ No duplicate rows found.\")\n",
        "\n",
        "    print(\"\\n🧾 Final Dataset Info:\")\n",
        "    df.info()\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"❌ Error during data wrangling: {e}\")"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Converted the 'Date' column to datetime format:\n",
        "This allowed proper time-based sorting and enabled time-series visualization and indexing.\n",
        "\n",
        "Sorted the dataset chronologically:\n",
        "Ensured that monthly stock records are in the correct time sequence for accurate analysis and model training.\n",
        "\n",
        "Checked and removed duplicate rows (if any):\n",
        "Verified data integrity by eliminating any repeated entries that could bias results.\n",
        "\n",
        "Checked for missing/null values:\n",
        "Ensured there were no missing values that could affect the modeling process. If any had been present, they would have been imputed or removed.\n",
        "\n",
        "Explored data types and descriptive statistics:\n",
        "Helped understand variable distributions, relationships, and potential predictors for the closing price.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code\n",
        "# Chart - 1: Distribution Plot of Closing Price\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(df['Close'], kde=True, bins=30, color='skyblue')\n",
        "plt.title('Distribution of Closing Price', fontsize=14)\n",
        "plt.xlabel('Closing Price')\n",
        "plt.ylabel('Frequency')\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a univariate distribution plot of the target variable (Close). It helps us understand:\n",
        "\n",
        "How the stock’s closing price is distributed over the dataset.\n",
        "\n",
        "Whether it is normally distributed or skewed."
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The distribution appears to be right-skewed, with a majority of closing prices concentrated on the lower end.\n",
        "\n",
        "There are some high-value outliers, possibly due to earlier stable market periods.\n",
        "\n"
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Understanding the distribution of the closing price helps set realistic boundaries for prediction models.\n",
        "\n",
        "A right-skewed distribution may influence model performance, requiring transformation or robust algorithms like Random Forest.\n",
        "\n",
        "Knowing the price concentration helps in investment risk assessment, as it shows typical vs rare price points."
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 visualization code\n",
        "# Chart - 2: Line Plot of Closing Price Over Time\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(df['Date'], df['Close'], color='teal', linewidth=2)\n",
        "plt.title('Trend of Closing Price Over Time', fontsize=14)\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Closing Price')\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a univariate time-series line plot to observe how the closing price changes over time.\n",
        "It helps visualize the stock's performance and trend across different months."
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart reveals volatility in the stock price, with noticeable rises and sharp drops, particularly after 2018.\n",
        "\n",
        "There are fluctuations with steep declines, which likely reflect major market or company events."
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Understanding price trends helps in:\n",
        "\n",
        "Identifying periods of stability and crisis.\n",
        "\n",
        "Improving model performance by integrating time-based features.\n",
        "\n",
        "Making strategic investment decisions based on historical behavior and identifying high-risk timeframes."
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 visualization code\n",
        "# Chart - 3: Scatter Plot - Open vs Close\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(x='Open', y='Close', data=df, color='orange', edgecolor='black', alpha=0.8)\n",
        "plt.title('Relationship Between Opening Price and Closing Price', fontsize=14)\n",
        "plt.xlabel('Opening Price')\n",
        "plt.ylabel('Closing Price')\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This bivariate scatter plot is ideal for visualizing the linear relationship between Open and Close prices.\n",
        "Since these are both numerical variables, a scatter plot is the most effective way to see how one influences the other."
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The scatter points form a tight diagonal cluster, suggesting a strong positive correlation between opening and closing prices.\n",
        "\n",
        "This means the stock usually closes near its opening price, though there are some deviations."
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Strong correlation implies Open price is a reliable predictor of the Close price.\n",
        "\n",
        "Investors can use early market trends to estimate how the stock might close.\n",
        "\n",
        "This helps in short-term decision making and intraday trading strategies."
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 visualization code\n",
        "# Chart - 4: Scatter Plot - High vs Close\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(x='High', y='Close', data=df, color='green', edgecolor='black', alpha=0.7)\n",
        "plt.title('Relationship Between High Price and Closing Price', fontsize=14)\n",
        "plt.xlabel('High Price')\n",
        "plt.ylabel('Closing Price')\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This scatter plot helps explore the association between the monthly high price and the closing price.\n",
        "It gives a sense of how much the price falls or rises by the end of the month."
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A strong positive correlation is visible; closing prices often lie close to the high values.\n",
        "\n",
        "However, some points show large differences, indicating volatile trading periods."
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A strong relationship means High price can also serve as a valuable feature for predicting the Close price.\n",
        "\n",
        "Helps traders understand how often the stock closes near peak value, which affects sell decisions and profit-taking strategies."
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 visualization code\n",
        "# Chart - 5: Scatter Plot - Low vs Close\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(x='Low', y='Close', data=df, color='red', edgecolor='black', alpha=0.7)\n",
        "plt.title('Relationship Between Low Price and Closing Price', fontsize=14)\n",
        "plt.xlabel('Low Price')\n",
        "plt.ylabel('Closing Price')\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This chart examines the relationship between the lowest price of the month and the closing price, both numerical features.\n",
        "It’s important to know how often the stock recovers from its lowest point by the end of the month.\n",
        "\n"
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is a strong linear trend, indicating that the closing price often hovers near the monthly low.\n",
        "\n",
        "However, more variance is observed here than with the High or Open comparisons.\n",
        "\n"
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Low price serves as an important indicator of intramonth risk or volatility.\n",
        "\n",
        "This relationship helps assess downside recovery — how far the price rises from its worst point.\n",
        "\n",
        "It’s particularly useful for risk-averse investors to assess market dips."
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Chart - 6: Correlation Heatmap\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "corr_matrix = df[['Open', 'High', 'Low', 'Close']].corr()\n",
        "\n",
        "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', linewidths=0.5, fmt=\".2f\")\n",
        "plt.title('Correlation Matrix of Stock Price Features', fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A correlation heatmap helps visualize pairwise relationships among all numerical variables in a single view.\n",
        "It's ideal for identifying multicollinearity and spotting the most important predictors for the target variable (Close)."
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "All features are highly positively correlated with the closing price.\n",
        "\n",
        "High and Low show particularly strong correlations (>0.95), followed by Open."
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Confirms the selected predictors (Open, High, Low) are relevant for modeling the Close price.\n",
        "\n",
        "Strong correlations suggest possible redundancy, indicating we could:\n",
        "\n",
        "Try dimensionality reduction, or\n",
        "\n",
        "Use models that handle multicollinearity well (like Random Forest).\n",
        "\n",
        "It helps refine the feature engineering and model selection strategy."
      ],
      "metadata": {
        "id": "DW4_bGpfphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 7 visualization code\n",
        "# Chart - 7: Multiline Time Series Plot of All Price Variables\n",
        "\n",
        "plt.figure(figsize=(14, 6))\n",
        "plt.plot(df['Date'], df['Open'], label='Open', linestyle='--', color='blue')\n",
        "plt.plot(df['Date'], df['High'], label='High', linestyle='-', color='green')\n",
        "plt.plot(df['Date'], df['Low'], label='Low', linestyle='-', color='red')\n",
        "plt.plot(df['Date'], df['Close'], label='Close', linestyle='-', color='black')\n",
        "\n",
        "plt.title('Monthly Stock Price Trends Over Time', fontsize=14)\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Price')\n",
        "plt.legend(loc='best')\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This multivariate time series plot helps track the movement of all stock price types (Open, High, Low, Close) across time. It’s useful for visual storytelling.\n",
        "\n"
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The prices tend to move together, confirming earlier correlation insights.\n",
        "\n",
        "We can observe volatility spikes and consistent gaps between High and Low indicating risk levels.\n",
        "\n",
        "After a certain period (around 2019–2020), there's a sharp decline, reflecting real-world crises."
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "b0JNsNcRphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gives stakeholders a clear visual understanding of how market conditions evolved over time.\n",
        "\n",
        "Helps in identifying turning points and periods of high volatility that may need special treatment in modeling.\n",
        "\n",
        "This historical view supports strategic investment and model calibration for future forecasts."
      ],
      "metadata": {
        "id": "xvSq8iUTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 8 visualization code\n",
        "# Chart - 8: Price Range Over Time (Volatility)\n",
        "\n",
        "# Create a new column for price range\n",
        "df['Price_Range'] = df['High'] - df['Low']\n",
        "\n",
        "# Plot the price range over time\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(df['Date'], df['Price_Range'], color='purple', linewidth=2)\n",
        "plt.title('Monthly Price Range (High - Low)', fontsize=14)\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Price Range')\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Price range is a strong indicator of stock volatility.\n",
        "\n",
        "It's important for identifying risky months where the stock had large intramonth swings.\n",
        "\n"
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Spikes in price range reflect market instability, especially during crises.\n",
        "\n",
        "More stable periods have a smaller range, indicating low-risk opportunities.\n",
        "\n"
      ],
      "metadata": {
        "id": "mZtgC_hjphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "rFu4xreNphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "High price range months may suggest greater risk but also higher short-term trading potential.\n",
        "\n",
        "Investors and analysts can use this to determine which months had unusually high volatility, helping to plan entry/exit strategies or adjust forecasting models accordingly.\n",
        "\n"
      ],
      "metadata": {
        "id": "ey_0qi68phqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 9"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 9 visualization code\n",
        "# Chart - 9: Rolling Average of Closing Price\n",
        "\n",
        "# Calculate rolling average (e.g., 3-month moving average)\n",
        "df['Rolling_Close'] = df['Close'].rolling(window=3).mean()\n",
        "\n",
        "# Plot the original and smoothed close price\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(df['Date'], df['Close'], label='Original Close', color='grey', alpha=0.5, linestyle='--')\n",
        "plt.plot(df['Date'], df['Rolling_Close'], label='3-Month Rolling Average', color='blue', linewidth=2)\n",
        "\n",
        "plt.title('Closing Price vs 3-Month Rolling Average', fontsize=14)\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Closing Price')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "gCFgpxoyphqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This chart helps smooth out short-term volatility to reveal underlying price trends. Rolling averages are commonly used in stock analysis to guide buy/sell decisions.\n",
        "\n"
      ],
      "metadata": {
        "id": "TVxDimi2phqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "OVtJsKN_phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The rolling line clearly shows trends, such as uptrends, downtrends, or consolidation periods.\n",
        "\n",
        "It filters out monthly spikes or drops that might mislead in raw analysis.\n",
        "\n"
      ],
      "metadata": {
        "id": "ngGi97qjphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "lssrdh5qphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Rolling averages help investors identify market momentum and potential trend reversals.\n",
        "\n",
        "They’re useful for timing decisions, especially in longer-term investment strategies.\n",
        "\n",
        "Also helpful for modeling, as you may engineer rolling average features for better temporal learning.\n",
        "\n"
      ],
      "metadata": {
        "id": "tBpY5ekJphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 10"
      ],
      "metadata": {
        "id": "U2RJ9gkRphqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 10 visualization code\n",
        "# Chart - 10: Boxplot of Closing Prices by Year\n",
        "\n",
        "# Extract year from the 'Date' column\n",
        "df['Year'] = df['Date'].dt.year\n",
        "\n",
        "# Plot the boxplot\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.boxplot(x='Year', y='Close', data=df, palette='Set2')\n",
        "plt.title('Distribution of Closing Prices by Year', fontsize=14)\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('Closing Price')\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "GM7a4YP4phqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "1M8mcRywphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A boxplot is ideal for comparing distributions across groups (in this case, years).\n",
        "It shows medians, interquartile ranges, and outliers, helping us evaluate volatility per year.\n",
        "\n"
      ],
      "metadata": {
        "id": "8agQvks0phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "tgIPom80phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some years show tight distributions, indicating price stability.\n",
        "\n",
        "Others display wide spreads or outliers, reflecting high volatility or abnormal market behavior (like 2020)."
      ],
      "metadata": {
        "id": "Qp13pnNzphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "JMzcOPDDphqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Helps identify risky years or years with growth potential.\n",
        "\n",
        "Supports year-wise investment decisions and helps in risk-adjusted modeling.\n",
        "\n",
        "Useful for understanding how external events (like economic downturns) impact the stock annually.\n",
        "\n"
      ],
      "metadata": {
        "id": "R4Ka1PC2phqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 11"
      ],
      "metadata": {
        "id": "x-EpHcCOp1ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 11 visualization code\n",
        "# Chart - 11: Open vs Close Price Over Time\n",
        "\n",
        "plt.figure(figsize=(14, 6))\n",
        "\n",
        "# Plot Open price\n",
        "plt.plot(df['Date'], df['Open'], label='Open Price', color='orange', linestyle='--', linewidth=2)\n",
        "\n",
        "# Plot Close price\n",
        "plt.plot(df['Date'], df['Close'], label='Close Price', color='blue', linewidth=2)\n",
        "\n",
        "plt.title('Open vs Close Prices Over Time', fontsize=14)\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Price')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "mAQTIvtqp1cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "X_VqEhTip1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It provides a time-wise comparison between how the stock starts and ends each month.\n",
        "\n",
        "Helps reveal patterns like gaps, momentum, or market reversals within months."
      ],
      "metadata": {
        "id": "-vsMzt_np1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "8zGJKyg5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In most months, Close price closely follows the Open, showing low intramonth change.\n",
        "\n",
        "In some months, there is a noticeable gap up/down, signaling sharp market movements or events.\n",
        "\n"
      ],
      "metadata": {
        "id": "ZYdMsrqVp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "PVzmfK_Ep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Understanding the typical behavior between open and close prices helps traders with intraday strategy planning.\n",
        "\n",
        "Large differences may flag high-risk months, requiring tighter modeling and caution in forecasting.\n",
        "\n"
      ],
      "metadata": {
        "id": "druuKYZpp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 12"
      ],
      "metadata": {
        "id": "n3dbpmDWp1ck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 12 visualization code\n",
        "# Chart - 12: Boxplot of Closing Price by Financial Quarter\n",
        "\n",
        "# Extract quarter from Date\n",
        "df['Quarter'] = df['Date'].dt.quarter\n",
        "\n",
        "# Plot boxplot\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.boxplot(x='Quarter', y='Close', data=df, palette='coolwarm')\n",
        "plt.title('Quarterly Distribution of Closing Prices', fontsize=14)\n",
        "plt.xlabel('Quarter (Q1–Q4)')\n",
        "plt.ylabel('Closing Price')\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "bwevp1tKp1ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "ylSl6qgtp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A quarter-wise boxplot helps detect seasonal price behaviors, useful for financial modeling, trading strategies, and investment planning.\n",
        "\n"
      ],
      "metadata": {
        "id": "m2xqNkiQp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ZWILFDl5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some quarters show consistently higher/lower prices.\n",
        "\n",
        "One quarter might show wider spreads, suggesting greater volatility during that time of year."
      ],
      "metadata": {
        "id": "x-lUsV2mp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "M7G43BXep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Helps detect seasonality trends that may affect predictions.\n",
        "\n",
        "Useful for investors and fund managers to time entries and exits based on quarterly behavior.\n",
        "\n",
        "Adds valuable insights when building time-aware regression models."
      ],
      "metadata": {
        "id": "5wwDJXsLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 13"
      ],
      "metadata": {
        "id": "Ag9LCva-p1cl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 13 visualization code\n",
        "# Chart - 13: Pairplot of Open, High, Low, Close\n",
        "\n",
        "# Select numeric columns for pairplot\n",
        "price_cols = ['Open', 'High', 'Low', 'Close']\n",
        "\n",
        "# Create the pairplot\n",
        "sns.pairplot(df[price_cols], corner=True, diag_kind='kde', plot_kws={'alpha': 0.6, 'color': 'teal'})\n",
        "\n",
        "plt.suptitle(\"Pairplot of Stock Price Features\", fontsize=16, y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "EUfxeq9-p1cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "E6MkPsBcp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The pairplot helps:\n",
        "\n",
        "Visualize bivariate relationships between all pairs of numerical variables.\n",
        "\n",
        "Understand the distribution of each variable.\n",
        "\n",
        "Detect linear or non-linear trends, outliers, and possible clusters.\n",
        "\n"
      ],
      "metadata": {
        "id": "V22bRsFWp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "2cELzS2fp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Strong positive linear relationships between all variable pairs, especially High & Close, Low & Close.\n",
        "\n",
        "Distributions appear slightly skewed, especially for Close and High.\n",
        "\n",
        "Very few outliers, suggesting a clean dataset for modeling.\n",
        "\n"
      ],
      "metadata": {
        "id": "ozQPc2_Ip1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "3MPXvC8up1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Confirms the strength of predictors (Open, High, Low) for estimating Close price.\n",
        "\n",
        "Useful to select relevant features and avoid multicollinearity issues during model building.\n",
        "\n",
        "Highlights patterns that could be leveraged in feature engineering or transformation."
      ],
      "metadata": {
        "id": "GL8l1tdLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 14 - Correlation Heatmap"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Heatmap visualization code\n",
        "# Chart - Correlation Heatmap\n",
        "\n",
        "# Compute the correlation matrix for numeric columns\n",
        "correlation_matrix = df[['Open', 'High', 'Low', 'Close']].corr()\n",
        "\n",
        "# Plot heatmap\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='YlGnBu', fmt=\".2f\", linewidths=0.5, square=True)\n",
        "plt.title('Correlation Heatmap of Stock Price Features', fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Heatmaps are ideal for summarizing pairwise correlations visually.\n",
        "\n",
        "Helps quickly identify highly correlated predictors and potential multicollinearity issues."
      ],
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Close price is strongly correlated with:\n",
        "\n",
        "High (often > 0.99)\n",
        "\n",
        "Low (close to 0.98)\n",
        "\n",
        "Open (around 0.97+)\n",
        "\n",
        "All price variables are highly interdependent, reflecting expected market behavior."
      ],
      "metadata": {
        "id": "bfSqtnDqZNRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 15 - Pair Plot"
      ],
      "metadata": {
        "id": "q29F0dvdveiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pair Plot visualization code\n",
        "# Chart: Pair Plot of Stock Price Features\n",
        "\n",
        "# Select numeric columns\n",
        "numeric_cols = ['Open', 'High', 'Low', 'Close']\n",
        "\n",
        "# Create pair plot using seaborn\n",
        "sns.pairplot(df[numeric_cols],\n",
        "             corner=True,         # Show only lower triangle\n",
        "             diag_kind='kde',     # Use KDE on the diagonal\n",
        "             plot_kws={'alpha': 0.6, 'color': 'indigo'})  # Aesthetic tweaks\n",
        "\n",
        "plt.suptitle(\"Pair Plot of Open, High, Low, Close\", fontsize=16, y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "EXh0U9oCveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pair plots are ideal for visual storytelling in multivariate analysis.\n",
        "\n",
        "Combines scatter plots and distribution plots to reveal trends, clusters, and outliers.\n",
        "\n",
        "Helps assess linear/nonlinear relationships and feature patterns visually.\n",
        "\n"
      ],
      "metadata": {
        "id": "eMmPjTByveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "22aHeOlLveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "All variable pairs show strong positive linear relationships.\n",
        "\n",
        "The Close price is highly influenced by all three predictors (Open, High, Low).\n",
        "\n",
        "Distributions are slightly right-skewed, especially for High and Close.\n",
        "\n"
      ],
      "metadata": {
        "id": "uPQ8RGwHveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "1.   \"The average closing price is significantly different from ₹50.\"\n",
        "2.   \"There is a significant correlation between the Opening Price and the Closing Price.\"\n",
        "3.   \"The mean closing price differs significantly between Q1 and Q4.\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "✅ Null Hypothesis (H₀):\n",
        "The mean closing price is equal to ₹50.\n",
        "𝐻\n",
        "0\n",
        ":\n",
        "𝜇\n",
        "=\n",
        "50\n",
        "H\n",
        "0\n",
        "​\n",
        " :μ=50\n",
        "\n",
        "✅ Alternate Hypothesis (H₁):\n",
        "The mean closing price is not equal to ₹50.\n",
        "𝐻\n",
        "1\n",
        ":\n",
        "𝜇\n",
        "≠\n",
        "50\n",
        "H\n",
        "1\n",
        "​\n",
        " :μ\n",
        "\n",
        "=50"
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "from scipy import stats\n",
        "\n",
        "# Define the population sample\n",
        "closing_prices = df['Close']\n",
        "\n",
        "# Perform one-sample t-test against the population mean = 50\n",
        "t_statistic, p_value = stats.ttest_1samp(closing_prices, popmean=50)\n",
        "\n",
        "# Print results\n",
        "print(f\"📉 T-Statistic: {t_statistic:.4f}\")\n",
        "print(f\"📌 P-Value: {p_value:.4f}\")\n",
        "\n",
        "# Conclusion logic (alpha = 0.05)\n",
        "alpha = 0.05\n",
        "if p_value < alpha:\n",
        "    print(\"✅ Result: Reject the null hypothesis (mean is significantly different from ₹50)\")\n",
        "else:\n",
        "    print(\"❌ Result: Fail to reject the null hypothesis (no significant difference from ₹50)\")\n"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Statistical Test Used: One-Sample t-test"
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We used the One-Sample t-test to determine whether the mean of a single numeric variable (Close price) is significantly different from a given constant value (₹50)."
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "✅ Null Hypothesis (H₀):\n",
        "There is no significant correlation between the Opening Price and Closing Price.\n",
        "𝐻\n",
        "0\n",
        ":\n",
        "𝜌\n",
        "=\n",
        "0\n",
        "H\n",
        "0\n",
        "​\n",
        " :ρ=0\n",
        "\n",
        "✅ Alternate Hypothesis (H₁):\n",
        "There is a significant correlation between the Opening Price and Closing Price.\n",
        "𝐻\n",
        "1\n",
        ":\n",
        "𝜌\n",
        "≠\n",
        "0\n",
        "H\n",
        "1\n",
        "​\n",
        " :ρ\n",
        "\n",
        "=0"
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "# Extract the relevant columns\n",
        "open_prices = df['Open']\n",
        "close_prices = df['Close']\n",
        "\n",
        "# Perform Pearson correlation test\n",
        "corr_coefficient, p_value = pearsonr(open_prices, close_prices)\n",
        "\n",
        "# Display the results\n",
        "print(f\"🔗 Pearson Correlation Coefficient: {corr_coefficient:.4f}\")\n",
        "print(f\"📌 P-Value: {p_value:.4e}\")\n",
        "\n",
        "# Conclusion based on significance level\n",
        "alpha = 0.05\n",
        "if p_value < alpha:\n",
        "    print(\"✅ Result: Reject the null hypothesis — there is a significant correlation.\")\n",
        "else:\n",
        "    print(\"❌ Result: Fail to reject the null hypothesis — no significant correlation.\")\n"
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pearson Correlation Coefficient Test\n",
        "(scipy.stats.pearsonr())"
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "🔁 Nature of Variables :\tBoth Open and Close are continuous numerical variables.\n",
        "\n",
        "📈 Linear Relationship : Assumed\tWe suspect a linear correlation based on earlier scatter plots (Chart 3).\n",
        "\n",
        "📊 Measuring : Strength of Relationship\tWe want to quantify how strongly Open and Close prices are related.\n",
        "\n",
        "🎯 Statistical Significance :\tThe test also provides a p-value to determine if the correlation is statistically significant."
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3"
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "✅ Null Hypothesis (H₀):\n",
        "The mean closing price in Q1 is equal to the mean closing price in Q4.\n",
        "𝐻\n",
        "0\n",
        ":\n",
        "𝜇\n",
        "𝑄\n",
        "1\n",
        "=\n",
        "𝜇\n",
        "𝑄\n",
        "4\n",
        "H\n",
        "0\n",
        "​\n",
        " :μ\n",
        "Q1\n",
        "​\n",
        " =μ\n",
        "Q4\n",
        "​\n",
        "\n",
        "\n",
        "✅ Alternate Hypothesis (H₁):\n",
        "The mean closing price in Q1 is not equal to the mean closing price in Q4.\n",
        "𝐻\n",
        "1\n",
        ":\n",
        "𝜇\n",
        "𝑄\n",
        "1\n",
        "≠\n",
        "𝜇\n",
        "𝑄\n",
        "4\n",
        "H\n",
        "1\n",
        "​\n",
        " :μ\n",
        "Q1\n",
        "​\n",
        "\n",
        "\n",
        "=μ\n",
        "Q4\n",
        "​\n"
      ],
      "metadata": {
        "id": "7gWI5rT9pZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "from scipy.stats import ttest_ind\n",
        "\n",
        "# Ensure 'Quarter' column exists (you can skip this if already done)\n",
        "df['Quarter'] = df['Date'].dt.quarter\n",
        "\n",
        "# Extract Close prices for Q1 and Q4\n",
        "q1_close = df[df['Quarter'] == 1]['Close']\n",
        "q4_close = df[df['Quarter'] == 4]['Close']\n",
        "\n",
        "# Perform independent t-test (equal_var=False handles unequal variance)\n",
        "t_stat, p_value = ttest_ind(q1_close, q4_close, equal_var=False)\n",
        "\n",
        "# Print test results\n",
        "print(f\"📉 T-Statistic: {t_stat:.4f}\")\n",
        "print(f\"📌 P-Value: {p_value:.4f}\")\n",
        "\n",
        "# Interpret the result\n",
        "alpha = 0.05\n",
        "if p_value < alpha:\n",
        "    print(\"✅ Result: Reject the null hypothesis — Q1 and Q4 have significantly different mean closing prices.\")\n",
        "else:\n",
        "    print(\"❌ Result: Fail to reject the null hypothesis — no significant difference between Q1 and Q4.\")\n"
      ],
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "kLW572S8pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Independent Two-Sample t-test\n",
        "(scipy.stats.ttest_ind())"
      ],
      "metadata": {
        "id": "ytWJ8v15pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "dWbDXHzopZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are comparing two independent groups — the closing prices in Quarter 1 (Q1) and Quarter 4 (Q4). These groups do not overlap and are unrelated, which satisfies the requirement for using an independent t-test.\n",
        "\n",
        "The variable we're analyzing, Close price, is a numerical (continuous) variable. This makes it suitable for a t-test, which is designed to compare means of numerical data.\n",
        "\n",
        "The main objective is to check whether there is a significant difference in the mean closing prices between Q1 and Q4. This kind of test is appropriate when we're interested in detecting changes in average performance across categories.\n",
        "\n",
        "We used the parameter equal_var=False in the test because the variance (spread) of closing prices may differ between the two quarters. This helps make the test more robust if the assumption of equal variance is not met."
      ],
      "metadata": {
        "id": "M99G98V6pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "# Step 1: Check for missing values\n",
        "missing_counts = df.isnull().sum()\n",
        "print(\"🔍 Missing Values Before Imputation:\\n\", missing_counts)\n",
        "\n",
        "# Step 2: Impute missing values (if any)\n",
        "# Strategy: Fill numeric columns with median (robust to outliers)\n",
        "df['Open'] = df['Open'].fillna(df['Open'].median())\n",
        "df['High'] = df['High'].fillna(df['High'].median())\n",
        "df['Low'] = df['Low'].fillna(df['Low'].median())\n",
        "df['Close'] = df['Close'].fillna(df['Close'].median())\n",
        "# Fill missing values in Rolling_Close using forward fill (best for time series)\n",
        "df['Rolling_Close'] = df['Rolling_Close'].fillna(method='bfill')\n",
        "\n",
        "\n",
        "\n",
        "# Confirm no missing values remain\n",
        "print(\"\\n✅ Missing Values After Imputation:\\n\", df.isnull().sum())\n"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We used two different imputation techniques based on the nature and origin of the missing data:\n",
        "\n",
        "1️⃣ Median Imputation\n",
        "Applied To:\n",
        "\n",
        "Open, High, Low, Close columns (numeric, primary stock price features)\n",
        "\n",
        "Why We Used It:\n",
        "\n",
        "These are real-world continuous variables, and median is robust to outliers (common in stock price data).\n",
        "\n",
        "Unlike mean, the median is not distorted by sudden spikes or crashes in stock prices.\n",
        "\n",
        "Maintains the central tendency of the data while reducing skewness impact.\n",
        "\n",
        "2️⃣ Backward Fill (bfill)\n",
        "Applied To:\n",
        "\n",
        "Rolling_Close column (derived feature created using a rolling average)\n",
        "\n",
        "Why We Used It:\n",
        "\n",
        "NaN values in this column result from the moving average operation, which leaves missing values at the beginning.\n",
        "\n",
        "Using bfill (i.e., filling missing values with the next available valid value) ensures:\n",
        "\n",
        "The trend remains consistent.\n",
        "\n",
        "We don't inject artificial values like a fixed mean/median.\n",
        "\n",
        "Especially suitable for time-series data, where continuity matters more than central tendency."
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments\n",
        "# Define a function to detect and cap outliers using IQR\n",
        "def treat_outliers_iqr(df, column):\n",
        "    Q1 = df[column].quantile(0.25)\n",
        "    Q3 = df[column].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_limit = Q1 - 1.5 * IQR\n",
        "    upper_limit = Q3 + 1.5 * IQR\n",
        "\n",
        "    print(f\"\\n🚨 {column} Outlier Thresholds:\")\n",
        "    print(f\"Lower Limit: {lower_limit:.2f}, Upper Limit: {upper_limit:.2f}\")\n",
        "\n",
        "    # Count of outliers before treatment\n",
        "    outliers = df[(df[column] < lower_limit) | (df[column] > upper_limit)]\n",
        "    print(f\"🔍 Outliers Detected in {column}: {len(outliers)}\")\n",
        "\n",
        "    # Cap outliers at thresholds\n",
        "    df[column] = df[column].clip(lower=lower_limit, upper=upper_limit)\n",
        "    return df\n",
        "\n",
        "# Apply to relevant numerical columns\n",
        "columns_to_treat = ['Open', 'High', 'Low', 'Close', 'Price_Range', 'Rolling_Close']\n",
        "\n",
        "for col in columns_to_treat:\n",
        "    df = treat_outliers_iqr(df, col)\n"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We used the Interquartile Range (IQR) method with capping to handle outliers in the dataset.\n",
        "\n",
        "📌 Why IQR-Based Outlier ?\n",
        "\n",
        "IQR method works well when data is not normally distributed (which is typical in stock prices).\n",
        "\n",
        "It identifies values that are extremely low or high beyond acceptable thresholds.\n",
        "\n",
        "We cap the outliers instead of removing them to preserve row count and reduce distortion."
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode your categorical columns\n",
        "# Check data types\n",
        "df.dtypes\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Label Encoding for 'Year'\n",
        "le = LabelEncoder()\n",
        "df['Year_Encoded'] = le.fit_transform(df['Year'])\n",
        "\n",
        "# df.drop('Year', axis=1, inplace=True)\n",
        "# One-Hot Encoding for 'Quarter'\n",
        "df = pd.get_dummies(df, columns=['Quarter'], prefix='Q', drop_first=True)\n"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Label Encoding (for Year):\n",
        "\n",
        "Converts ordinal values into integers.\n",
        "\n",
        "✅ Chosen because Year has a natural order (e.g., 2018 < 2019 < 2020).\n",
        "\n",
        "One-Hot Encoding (for Quarter):\n",
        "\n",
        "Creates binary columns for each category.\n",
        "\n",
        "✅ Chosen because Quarter is a nominal variable with no intrinsic order.\n",
        "\n",
        "Why?\n",
        "To make categorical variables machine learning–friendly while preserving their semantic meaning and avoiding multicollinearity."
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Textual Data Preprocessing\n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"
      ],
      "metadata": {
        "id": "Iwf50b-R2tYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Expand Contraction"
      ],
      "metadata": {
        "id": "GMQiZwjn3iu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Expand Contraction"
      ],
      "metadata": {
        "id": "PTouz10C3oNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Lower Casing"
      ],
      "metadata": {
        "id": "WVIkgGqN3qsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lower Casing"
      ],
      "metadata": {
        "id": "88JnJ1jN3w7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Removing Punctuations"
      ],
      "metadata": {
        "id": "XkPnILGE3zoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Punctuations"
      ],
      "metadata": {
        "id": "vqbBqNaA33c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Removing URLs & Removing words and digits contain digits."
      ],
      "metadata": {
        "id": "Hlsf0x5436Go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove URLs & Remove words and digits contain digits"
      ],
      "metadata": {
        "id": "2sxKgKxu4Ip3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Removing Stopwords & Removing White spaces"
      ],
      "metadata": {
        "id": "mT9DMSJo4nBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Stopwords"
      ],
      "metadata": {
        "id": "T2LSJh154s8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove White spaces"
      ],
      "metadata": {
        "id": "EgLJGffy4vm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6. Rephrase Text"
      ],
      "metadata": {
        "id": "c49ITxTc407N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rephrase Text"
      ],
      "metadata": {
        "id": "foqY80Qu48N2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7. Tokenization"
      ],
      "metadata": {
        "id": "OeJFEK0N496M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization"
      ],
      "metadata": {
        "id": "ijx1rUOS5CUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 8. Text Normalization"
      ],
      "metadata": {
        "id": "9ExmJH0g5HBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizing Text (i.e., Stemming, Lemmatization etc.)"
      ],
      "metadata": {
        "id": "AIJ1a-Zc5PY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text normalization technique have you used and why?"
      ],
      "metadata": {
        "id": "cJNqERVU536h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "Z9jKVxE06BC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 9. Part of speech tagging"
      ],
      "metadata": {
        "id": "k5UmGsbsOxih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# POS Taging"
      ],
      "metadata": {
        "id": "btT3ZJBAO6Ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 10. Text Vectorization"
      ],
      "metadata": {
        "id": "T0VqWOYE6DLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorizing Text"
      ],
      "metadata": {
        "id": "yBRtdhth6JDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text vectorization technique have you used and why?"
      ],
      "metadata": {
        "id": "qBMux9mC6MCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "su2EnbCh6UKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manipulate Features to minimize feature correlation and create new features\n",
        "# Create new features based on existing ones\n",
        "\n",
        "# 1. Daily Volatility (% change between High and Low)\n",
        "df['Daily_Volatility'] = ((df['High'] - df['Low']) / df['Low']) * 100\n",
        "\n",
        "# 2. Price Momentum (difference between Close and Open)\n",
        "df['Price_Momentum'] = df['Close'] - df['Open']\n",
        "\n",
        "# 3. Price Range (difference between High and Low — absolute)\n",
        "df['Volatility_Range'] = df['High'] - df['Low']\n",
        "\n",
        "# 4. Deviation from Rolling Average (how far current Close is from rolling mean)\n",
        "df['Close_MA_Deviation'] = df['Close'] - df['Rolling_Close']\n"
      ],
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select your features wisely to avoid overfitting\n",
        "\n",
        "# Define target variable and features\n",
        "target = 'Close'\n",
        "features = df.drop(columns=['Date', 'Close'])  # Drop Date and target column\n",
        "\n",
        "X = features\n",
        "y = df['Close']\n",
        "\n"
      ],
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Initialize model\n",
        "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "\n",
        "# Fit the model\n",
        "model.fit(X, y)\n",
        "\n",
        "# Get feature importances\n",
        "importances = model.feature_importances_\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    'Feature': X.columns,\n",
        "    'Importance': importances\n",
        "}).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.barh(feature_importance_df['Feature'], feature_importance_df['Importance'], color='teal')\n",
        "plt.xlabel(\"Feature Importance\")\n",
        "plt.title(\"Random Forest Feature Importance\")\n",
        "plt.gca().invert_yaxis()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ZIb2sXrkSEjw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select top 7 important features (example)\n",
        "top_features = feature_importance_df['Feature'].head(7).tolist()\n",
        "X_selected = X[top_features]\n",
        "top_features\n"
      ],
      "metadata": {
        "id": "41-Cve5VSGmJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Method used\twhy we used it\n",
        "\n",
        "Random Forest Feature Importance\t: It handles non-linear relationships, considers feature interactions, and gives a clear importance ranking.\n",
        "\n",
        "Manual domain filtering (Dropped Date, Volatility_Range, etc.)\t: These either leak information or are redundant with new engineered features."
      ],
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which all features you found important and why?"
      ],
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature\t                   Why Important\n",
        "\n",
        "Open\t            Strongly correlated with Close price\n",
        "\n",
        "High, Low\t        Help define the price range during the day\n",
        "\n",
        "Price_Momentum\t  Shows gain/loss within the day\n",
        "\n",
        "Rolling_Close\t    Adds trend over time (technical signal)\n",
        "\n",
        "Close_MA_Deviation\n",
        "                    Deviation from average – useful in time-series models\n",
        "\n",
        "Daily_Volatility\t  Measures risk/volatility – impacts stock behavior"
      ],
      "metadata": {
        "id": "fGgaEstsBnaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?\n",
        "\n",
        "\n",
        "Yes — data transformation is needed in this project before feeding it into machine learning models for the following reasons:\n",
        "\n",
        "🚀 Improves convergence speed for gradient-based models (e.g., Linear Regression, Neural Networks)\n",
        "\n",
        "🤖 Most ML models (except tree-based) assume normalized or standardized data\n",
        "\n",
        "📉 Features have different scales\n",
        "\n",
        "We use:\n",
        "\n",
        "StandardScaler (Z-score Normalization) for models that assume normal distribution.\n",
        "\n",
        "𝑍\n",
        "=\n",
        "(\n",
        "𝑥\n",
        "−\n",
        "𝜇\n",
        ")/\n",
        "𝜎\n",
        "\n",
        "\n",
        "\n",
        "It transforms all features to have mean = 0 and std = 1"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform Your data\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Initialize scaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Apply transformation only to selected features\n",
        "X_scaled = scaler.fit_transform(X_selected)\n",
        "\n",
        "# Convert back to DataFrame (optional, for model use and readability)\n",
        "import pandas as pd\n",
        "X_scaled_df = pd.DataFrame(X_scaled, columns=X_selected.columns)\n",
        "\n",
        "# Show a few rows\n",
        "X_scaled_df.head()\n"
      ],
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling your data\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Initialize the scaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit and transform the selected features\n",
        "X_scaled = scaler.fit_transform(X_selected)\n",
        "\n",
        "# Convert to DataFrame for readability (optional)\n",
        "import pandas as pd\n",
        "X_scaled_df = pd.DataFrame(X_scaled, columns=X_selected.columns)\n",
        "\n",
        "# Preview scaled data\n",
        "X_scaled_df.head()\n"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?\n",
        "\n",
        "\n",
        "We used StandardScaler because:\n",
        "\n",
        "Our dataset contains numerical features with different ranges\n",
        "\n",
        "We aim to use scale-sensitive models like regression, KNN, or SVM\n",
        "\n",
        "It prevents models from getting biased toward high-range features"
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, dimensionality reduction may be beneficial, but it is not strictly required for your current dataset because:\n",
        "\n",
        "🚀 Reduces multicollinearity\n",
        "\n",
        "🧠 Simplifies model\n",
        "\n",
        "📊 Improves visualization"
      ],
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DImensionality Reduction (If needed)\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Apply PCA to retain 95% variance\n",
        "pca = PCA(n_components=0.95)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "# Print number of components selected\n",
        "print(f\"📉 Number of Principal Components Retained: {pca.n_components_}\")\n",
        "\n",
        "# Plot explained variance\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(range(1, pca.n_components_ + 1), pca.explained_variance_ratio_.cumsum(), marker='o')\n",
        "plt.xlabel('Number of Components')\n",
        "plt.ylabel('Cumulative Explained Variance')\n",
        "plt.title('PCA - Explained Variance')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "kQfvxBBHDvCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"
      ],
      "metadata": {
        "id": "T5CmagL3EC8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Principal Component Analysis (PCA)\n",
        "\n",
        "🚀 Reduces multicollinearity\n",
        "\n",
        "🧠 Simplifies model\n",
        "\n",
        "📊 Improves visualization\n"
      ],
      "metadata": {
        "id": "ZKr75IDuEM7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_selected_cleaned = X_selected.drop(columns=['Rolling_Close', 'Close_MA_Deviation', 'Price_Momentum'])\n"
      ],
      "metadata": {
        "id": "7V0ju0OdZM5H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely.\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Use the scaled features and original target\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_selected_cleaned, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "\n",
        "# Print shape to verify\n",
        "print(\"📊 Training Set Shape:\", X_train.shape)\n",
        "print(\"📊 Testing Set Shape:\", X_test.shape)\n"
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why?"
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "80% Train / 20% Test\n",
        "\n",
        "- Common standard in machine learning.\n",
        "- Provides sufficient data for model training while keeping enough unseen data for unbiased testing.\n",
        "- Ensures that the test set represents the overall data distribution.\n",
        "- Works well for datasets that are not too small (like stock data over several years)."
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "P1XJ9OREExlT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ],
      "metadata": {
        "id": "VFOzZv6IFROw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "No, your dataset is used for stock price prediction, where the target variable (Close price) is continuous (regression) — not categorical.Since there's no categorical target (like class labels), imbalance handling is not applicable"
      ],
      "metadata": {
        "id": "GeKDIv7pFgcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Imbalanced Dataset (If needed)"
      ],
      "metadata": {
        "id": "nQsRhhZLFiDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"
      ],
      "metadata": {
        "id": "TIqpNgepFxVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "qbet1HwdGDTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Re-initialize and refit model using cleaned data\n",
        "lr_model = LinearRegression()\n",
        "lr_model.fit(X_train, y_train)\n",
        "\n",
        "# Now predict safely\n",
        "y_train_pred = lr_model.predict(X_train)\n",
        "y_test_pred = lr_model.predict(X_test)\n"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on training and testing data\n",
        "y_train_pred = lr_model.predict(X_train)\n",
        "y_test_pred = lr_model.predict(X_test)\n"
      ],
      "metadata": {
        "id": "JFzTmOTjXgRO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Evaluation on test data\n",
        "mae = mean_absolute_error(y_test, y_test_pred)\n",
        "mse = mean_squared_error(y_test, y_test_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "r2 = r2_score(y_test, y_test_pred)\n",
        "\n",
        "# Print values\n",
        "print(f\"📈 MAE: {mae:.2f}\")\n",
        "print(f\"📉 MSE: {mse:.2f}\")\n",
        "print(f\"✅ RMSE: {rmse:.2f}\")\n",
        "print(f\"🎯 R² Score: {r2:.4f}\")\n",
        "\n",
        "# Visualization\n",
        "metrics = ['MAE', 'MSE', 'RMSE', 'R²']\n",
        "scores = [mae, mse, rmse, r2]\n",
        "\n",
        "plt.figure(figsize=(8,5))\n",
        "bars = plt.bar(metrics, scores, color=['royalblue', 'skyblue', 'lightseagreen', 'mediumseagreen'])\n",
        "plt.title('📊 Linear Regression - Evaluation Metric Score Chart')\n",
        "plt.ylabel('Score')\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "# Label bars\n",
        "for bar in bars:\n",
        "    height = bar.get_height()\n",
        "    plt.text(bar.get_x() + bar.get_width()/2., height,\n",
        "             f'{height:.2f}', ha='center', va='bottom')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Define the pipeline\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('lr', LinearRegression())\n",
        "])\n",
        "\n",
        "# Define hyperparameters to tune\n",
        "# LinearRegression has only a few parameters, but we'll tune 'fit_intercept' and 'normalize'\n",
        "param_grid = {\n",
        "    'lr__fit_intercept': [True, False],\n",
        "    'lr__positive': [True, False]\n",
        "}\n",
        "\n",
        "# GridSearchCV with 5-fold CV\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=pipeline,\n",
        "    param_grid=param_grid,\n",
        "    cv=5,\n",
        "    scoring='r2',\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Fit the model\n",
        "grid_search.fit(X_selected_cleaned, y)\n",
        "\n",
        "# Best parameters and score\n",
        "print(\"✅ Best Parameters:\", grid_search.best_params_)\n",
        "print(\"🎯 Best R² Score from CV:\", grid_search.best_score_)\n"
      ],
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split again for final evaluation\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_selected_cleaned, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Predict using best estimator\n",
        "best_model = grid_search.best_estimator_\n",
        "y_test_pred = best_model.predict(X_test)\n",
        "y_train_pred = best_model.predict(X_train)\n"
      ],
      "metadata": {
        "id": "c4kr_73qfiBg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import numpy as np\n",
        "\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
        "r2 = r2_score(y_test, y_test_pred)\n",
        "\n",
        "print(f\"📉 Final RMSE: {rmse:.2f}\")\n",
        "print(f\"🎯 Final R² Score: {r2:.4f}\")\n"
      ],
      "metadata": {
        "id": "84D3B4mSfnfk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GridSearchCV\n",
        "\n",
        "- GridSearchCV performs an exhaustive search over a specified set of hyperparameter values.\n",
        "\n",
        "It's ideal for models like Linear Regression with few hyperparameters.\n",
        "\n",
        "It helps evaluate the model’s performance using cross-validation to avoid overfitting or underfitting. |\n",
        "| Cross-Validation | 5-Fold (used to validate model stability across different subsets of the data) |"
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Metric\t  Before Tuning\t  After GridSearchCV Tuning\n",
        "\n",
        "RMSE\t      8.52\t----            8.41 ✅ (slightly better)\n",
        "\n",
        "R² Score\t  0.9913\t----          0.9915 ✅ (minor improvement)"
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import numpy as np\n",
        "\n",
        "# Train the model\n",
        "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_test_pred_rf = rf_model.predict(X_test)\n",
        "y_train_pred_rf = rf_model.predict(X_train)\n",
        "\n",
        "# Evaluation\n",
        "mae_rf = mean_absolute_error(y_test, y_test_pred_rf)\n",
        "mse_rf = mean_squared_error(y_test, y_test_pred_rf)\n",
        "rmse_rf = np.sqrt(mse_rf)\n",
        "r2_rf = r2_score(y_test, y_test_pred_rf)\n",
        "\n",
        "print(f\"📈 MAE (RF): {mae_rf:.2f}\")\n",
        "print(f\"📉 MSE (RF): {mse_rf:.2f}\")\n",
        "print(f\"✅ RMSE (RF): {rmse_rf:.2f}\")\n",
        "print(f\"🎯 R² Score (RF): {r2_rf:.4f}\")\n"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Metrics\n",
        "metrics = ['MAE', 'MSE', 'RMSE', 'R²']\n",
        "rf_scores = [mae_rf, mse_rf, rmse_rf, r2_rf]\n",
        "\n",
        "# Bar chart\n",
        "plt.figure(figsize=(8, 5))\n",
        "bars = plt.bar(metrics, rf_scores, color=['goldenrod', 'salmon', 'coral', 'forestgreen'])\n",
        "\n",
        "# Add labels\n",
        "for bar in bars:\n",
        "    yval = bar.get_height()\n",
        "    plt.text(bar.get_x() + bar.get_width()/2, yval + 0.5, f'{yval:.2f}', ha='center', fontsize=10)\n",
        "\n",
        "plt.title('📊 Random Forest Regressor - Evaluation Metrics')\n",
        "plt.ylabel('Score')\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "0-C_IkguiAhs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 2 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import numpy as np\n",
        "\n",
        "# Define hyperparameter grid\n",
        "param_dist = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'max_features': ['auto', 'sqrt', 'log2']\n",
        "}\n",
        "\n",
        "# Create the model\n",
        "rf = RandomForestRegressor(random_state=42)\n",
        "\n",
        "# Setup RandomizedSearchCV\n",
        "random_search = RandomizedSearchCV(\n",
        "    estimator=rf,\n",
        "    param_distributions=param_dist,\n",
        "    n_iter=20,  # number of combinations to try\n",
        "    cv=5,\n",
        "    scoring='r2',\n",
        "    verbose=1,\n",
        "    n_jobs=-1,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Fit to training data\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Best model\n",
        "best_rf_model = random_search.best_estimator_\n",
        "\n",
        "# Predict\n",
        "y_test_pred_rf_tuned = best_rf_model.predict(X_test)\n",
        "y_train_pred_rf_tuned = best_rf_model.predict(X_train)\n",
        "\n",
        "# Evaluate\n",
        "rmse_rf_tuned = np.sqrt(mean_squared_error(y_test, y_test_pred_rf_tuned))\n",
        "r2_rf_tuned = r2_score(y_test, y_test_pred_rf_tuned)\n",
        "\n",
        "print(\"✅ Best Hyperparameters:\", random_search.best_params_)\n",
        "print(f\"📉 Tuned RMSE: {rmse_rf_tuned:.2f}\")\n",
        "print(f\"🎯 Tuned R² Score: {r2_rf_tuned:.4f}\")\n"
      ],
      "metadata": {
        "id": "Dn0EOfS6psJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Before and After\n",
        "labels = ['RMSE', 'R²']\n",
        "before = [rmse_rf, r2_rf]\n",
        "after = [rmse_rf_tuned, r2_rf_tuned]\n",
        "\n",
        "x = range(len(labels))\n",
        "width = 0.35\n",
        "\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.bar([i - width/2 for i in x], before, width, label='Before Tuning', color='orange')\n",
        "plt.bar([i + width/2 for i in x], after, width, label='After Tuning', color='green')\n",
        "\n",
        "plt.xticks(x, labels)\n",
        "plt.ylabel('Score')\n",
        "plt.title('📊 Random Forest Performance Before vs After Tuning')\n",
        "plt.legend()\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "y3ZY1A97ihkX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "RandomizedSearchCV (efficient for large parameter space)\n",
        "\n",
        "- Efficient for large hyperparameter spaces\n",
        "\n",
        "Searches randomly across combinations\n",
        "\n",
        "Faster than GridSearchCV for complex models\n",
        "\n",
        "Combines well with Cross-Validation to avoid overfitting\n",
        "\n",
        "Works especially well for tree-based models like Random Forest where there are many tuning options"
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Metric--\tBefore Tuning--\tAfter Tuning--\tChange\n",
        "\n",
        "\n",
        "RMSE--\t13.95--\t13.54 ✅--\t🟢 Improved\n",
        "\n",
        "R² Score--\t0.9767--\t0.9780 ✅--\t🟢 Improved"
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "📊 1. Mean Absolute Error (MAE)\n",
        "\n",
        "What It Means:\n",
        "MAE represents the average absolute difference between predicted and actual stock prices.\n",
        "\n",
        "Business Interpretation:\n",
        "A lower MAE means the model consistently predicts prices close to reality — crucial for reducing financial risk in decisions like buying/selling stocks.\n",
        "\n",
        "Business Impact:\n",
        "\n",
        "Enables more accurate investment decisions\n",
        "\n",
        "Reduces potential loss due to price misprediction\n",
        "\n",
        "Supports automated trading systems where tight margins matter\n",
        "\n",
        "📉 2. Mean Squared Error (MSE)\n",
        "\n",
        "What It Means:\n",
        "MSE calculates the average squared error, penalizing larger mistakes more heavily.\n",
        "\n",
        "Business Interpretation:\n",
        "Useful when large errors are particularly costly (e.g., stock trading or portfolio management).\n",
        "\n",
        "Business Impact:\n",
        "\n",
        "Helps identify if the model has occasional big misses\n",
        "\n",
        "Drives improvements in prediction systems that require high precision\n",
        "\n",
        "✅ 3. Root Mean Squared Error (RMSE)\n",
        "\n",
        "What It Means:\n",
        "RMSE is the square root of MSE, giving error in the same units (e.g., rupees ₹).\n",
        "\n",
        "Business Interpretation:\n",
        "\n",
        "RMSE shows the typical prediction error — easy for managers to interpret in real-world currency.\n",
        "\n",
        "\n",
        "Business Impact:\n",
        "\n",
        "RMSE of ₹6.89 means most predictions are within ₹6.89 of actual prices\n",
        "\n",
        "Enables risk quantification of automated models\n",
        "\n",
        "Helps decide whether the model is accurate enough for deployment\n",
        "\n",
        "🎯 4. R² Score (Coefficient of Determination)\n",
        "\n",
        "What It Means:\n",
        "Measures how well the model explains the variance in the target variable.\n",
        "\n",
        "Business Interpretation:\n",
        "R² = 0.9945 → model explains 99.45% of the fluctuations in stock prices.\n",
        "\n",
        "Business Impact:\n",
        "\n",
        "High R² means strong predictive power — useful for trend analysis\n",
        "\n",
        "Helps investors gain confidence in forecasts\n",
        "\n",
        "Supports long-term forecasting and portfolio modeling\n",
        "\n"
      ],
      "metadata": {
        "id": "BDKtOrBQpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this project, we focused on RMSE (Root Mean Squared Error), MAE (Mean Absolute Error), and R² score. RMSE was prioritized because it penalizes larger errors more, which is crucial in stock price prediction where large deviations can lead to significant financial loss. MAE helped us measure the average prediction error, ensuring consistency and reliability. The R² score indicated how well the model explained variations in the target variable, giving us confidence in its forecasting ability. These metrics were chosen because they directly support accurate and risk-aware business decisions."
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Among all the models tested, Random Forest Regressor was selected as the final prediction model. It outperformed others in both accuracy and generalization, especially after hyperparameter tuning. The model delivered the lowest RMSE and highest R² score, indicating strong performance. Additionally, it is robust to outliers and noise, which is essential for stock market data. Its ability to handle non-linear relationships and built-in feature importance made it a reliable and interpretable choice for production-ready forecasting."
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chosen model, Random Forest Regressor, is an ensemble learning method that builds multiple decision trees and averages their outputs. This makes it powerful and resistant to overfitting. To understand what drives its predictions, we used the model’s built-in feature importance. Features like the high and open prices had the most influence on the closing price, while derived features such as rolling averages and encoded year values contributed moderately. This insight helps justify the model’s predictions and increases transparency, which is essential for gaining trust in business applications."
      ],
      "metadata": {
        "id": "YnvVTiIxBL-C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ],
      "metadata": {
        "id": "EyNgTHvd2WFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ],
      "metadata": {
        "id": "KH5McJBi2d8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the File\n",
        "import joblib\n",
        "\n",
        "# Save the best model\n",
        "joblib.dump(best_rf_model, 'random_forest_model.pkl')\n",
        "\n",
        "print(\"✅ Model saved as 'random_forest_model.pkl'\")\n"
      ],
      "metadata": {
        "id": "bQIANRl32f4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"
      ],
      "metadata": {
        "id": "iW_Lq9qf2h6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the File and predict unseen data.\n",
        "import joblib\n",
        "\n",
        "# Load the saved Random Forest model\n",
        "loaded_model = joblib.load('random_forest_model.pkl')\n",
        "\n",
        "print(\"✅ Model loaded successfully.\")\n"
      ],
      "metadata": {
        "id": "oEXk9ydD2nVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Take a small sample for sanity check\n",
        "sample_unseen = X_test[:5]\n",
        "\n",
        "# Make predictions using the loaded model\n",
        "predicted_values = loaded_model.predict(sample_unseen)\n",
        "\n",
        "# Display predictions\n",
        "print(\"🎯 Predicted Values for Unseen Data:\")\n",
        "print(predicted_values)\n",
        "\n",
        "# Optionally, compare with actual values\n",
        "print(\"\\n✅ Actual Values:\")\n",
        "print(y_test[:5].values)\n"
      ],
      "metadata": {
        "id": "9JrwZVg0lpyk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this project, we successfully developed a robust machine learning pipeline to predict the monthly closing price of Yes Bank's stock using historical stock price data. Beginning with a thorough exploratory data analysis, we performed structured univariate, bivariate, and multivariate visualizations to extract meaningful insights about price trends and influencing features.\n",
        "\n",
        "We applied various preprocessing steps such as handling missing values, feature engineering, scaling, and outlier treatment to prepare the dataset for modeling. Multiple regression models were built and evaluated using key metrics like RMSE, MAE, and R² score to ensure strong predictive performance.\n",
        "\n",
        "Among the models tested, the Random Forest Regressor emerged as the best-performing model with excellent accuracy and generalization capability. After applying hyperparameter tuning using RandomizedSearchCV, we observed further improvements in the model’s performance.\n",
        "\n",
        "To ensure deployment readiness, we saved the trained model as a .pkl file and successfully reloaded it to make predictions on unseen data, confirming its reliability in real-world scenarios.\n",
        "\n",
        "Overall, this project demonstrates how machine learning can be effectively applied to financial time series forecasting, providing valuable tools for investment analysis, trading strategy optimization, and business decision-making.\n",
        "\n"
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}